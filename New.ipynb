{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc (word):\n",
    "    s = word.encode('utf-8')\n",
    "    s = s.decode('utf-8')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, lemma, lang, s=[]):\n",
    "        if lemma == None: self.lemma = ''\n",
    "        else: self.lemma = enc(lemma)\n",
    "        self.lang = lang\n",
    "        self.s = s\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.s:\n",
    "            if isinstance(self.s[0],list):\n",
    "                w = '['+'_'.join(['-'.join(i) for i in self.s])+']'\n",
    "            else:\n",
    "                w = '['+'-'.join(self.s)+']'\n",
    "        else:\n",
    "            w = '-'\n",
    "        return str(self.lang)+'$'+str(self.lemma)+'$'+str(w)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.lemma == other.lemma and self.lang == other.lang and (self.s == other.s or other.s in self.s or self.s in other.s)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if self.lang == other.lang:\n",
    "            if self.lemma == other.lemma:\n",
    "                s1 = set(self.s)\n",
    "                s2 = set(other.s)\n",
    "                if (not s1 - s2) and (s1&s2==s1) and (s2 - s1):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "    \n",
    "    def write(self, mode='mono'):\n",
    "        if mode == 'mono':\n",
    "            return self.lemma + '\\t' + '$'.join([str(i) for i in self.s])\n",
    "        elif mode == 'bi':\n",
    "            return self.lang + '\\t' +  self.lemma + '\\t' + '$'.join([str(i) for i in self.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_languages():\n",
    "    G = nx.Graph()\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix', '').split('-')\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    d = G.degree()\n",
    "    d = sorted(d, key=d.get, reverse=True)\n",
    "    #print (d)\n",
    "    with open('languages','w',encoding='utf-8') as f:\n",
    "        f.write('\\t'.join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "%time all_languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monodix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tags(list):\n",
    "    def __le__(self, other):\n",
    "        s1 = set(self)\n",
    "        s2 = set(other)\n",
    "        if not s1 - s2 and s1&s2==s1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        s1 = set(self)\n",
    "        s2 = set(other)\n",
    "        if (not s1 - s2) and (s1&s2==s1) and (s2 - s1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if set(self) == set(other):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '-'.join(self)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDict(dict):\n",
    "    def lemma(self, lemma):\n",
    "        self.lemma = lemma\n",
    "        \n",
    "class FilteredDict(dict):\n",
    "    def set_lang(self, lang):\n",
    "        self.lang = lang\n",
    "    \n",
    "    def lemma(self, lemma):\n",
    "        return self[self.lang+'_'+lemma]\n",
    "        \n",
    "    def add(self, word):\n",
    "        lemma = word.lang+'_'+word.lemma\n",
    "        tags = Tags(word.s)\n",
    "        if lemma in self:\n",
    "            if tags in self[lemma]:\n",
    "                self[lemma][tags] += 1\n",
    "            else:\n",
    "                self[lemma][tags] = 1\n",
    "        else:\n",
    "            self[lemma] = WordDict()\n",
    "            self[lemma].lemma(lemma)\n",
    "            self[lemma][tags] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_language_dict(lang):\n",
    "    dictionary = FilteredDict()\n",
    "    dictionary.set_lang(lang)\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix','').split('-')\n",
    "            if lang in pair:\n",
    "                if lang == pair[0]: side = 'l'\n",
    "                else: side = 'r'\n",
    "                try:\n",
    "                    with open (root+fl, 'r', encoding='utf-8') as d:\n",
    "                        t = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))     \n",
    "                    for word in parse_one(t, side, lang):\n",
    "                        dictionary.add(word)\n",
    "                except:\n",
    "                    pass\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = one_language_dict('afr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{prn-tn-m: 1, prn-tn-mf: 2, det-ind: 1, prn: 1}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['afr_almal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('almal', [[prn-tn-mf, prn], [det-ind], [prn-tn-m]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten(dictionary['afr_almal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(word_dict):\n",
    "    short = []\n",
    "    for i in sorted(word_dict, key=lambda x: (word_dict[x], -len(x)), reverse=True):\n",
    "        new = True\n",
    "        for key, j in enumerate(short):\n",
    "            inner = True\n",
    "            for key2, k in enumerate(j):\n",
    "                if (k < i) or (i < k): pass\n",
    "                else: inner = False\n",
    "            if inner: \n",
    "                short[key].append(i)\n",
    "                new = False\n",
    "        if new: short.append([i])\n",
    "    word = word_dict.lemma[4:]\n",
    "    return word, short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_word(word, lang):\n",
    "    if word.text: st = str(word.text)\n",
    "    else: st = ''\n",
    "    s = [i.attrib['n'] for i in word.findall('.//s')]\n",
    "    s = [i for i in s if i != '']\n",
    "    return Word(st, lang, s)\n",
    "\n",
    "def parse_one (tree, side, lang):\n",
    "    tree = tree.find('section')\n",
    "    for e in tree:\n",
    "        p = e.find('p')\n",
    "        if p:\n",
    "            word = one_word(p.find(side), lang)\n",
    "            yield word\n",
    "        else:\n",
    "            i = e.find('i')\n",
    "            if i:\n",
    "                word = one_word(i, lang)\n",
    "                yield word\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_nodes(dictionary):\n",
    "    for i in dictionary.keys():\n",
    "        word, tags = shorten(dictionary[i])\n",
    "        if '_' in word:\n",
    "            word = word.replace('_', ' ')\n",
    "        for tag in tags:\n",
    "            yield Word(word, dictionary.lang, Tags([i for i in tag if i != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monodix():\n",
    "    if not os.path.exists('./monodix/'):\n",
    "        os.makedirs('./monodix/')\n",
    "    with open('languages','r', encoding='utf-8') as f:\n",
    "        langs = f.read().split('\\t')\n",
    "    for lang in langs:\n",
    "        dictionary = one_language_dict(lang)\n",
    "        with open ('./monodix/'+lang+'.dix', 'w', encoding = 'utf-16') as f:\n",
    "            for i in dictionary_to_nodes(dictionary):\n",
    "                f.write (i.write(mode='mono')+'\\n')\n",
    "        logging.info(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-01 23:06:34,036 | INFO : eng\n",
      "2018-06-01 23:06:45,989 | INFO : spa\n",
      "2018-06-01 23:06:52,225 | INFO : fin\n",
      "2018-06-01 23:07:01,691 | INFO : epo\n",
      "2018-06-01 23:07:07,187 | INFO : rus\n",
      "2018-06-01 23:07:13,218 | INFO : ita\n",
      "2018-06-01 23:07:19,605 | INFO : fra\n",
      "2018-06-01 23:07:21,951 | INFO : pol\n",
      "2018-06-01 23:07:31,397 | INFO : cat\n",
      "2018-06-01 23:07:34,256 | INFO : kaz\n",
      "2018-06-01 23:07:35,907 | INFO : tur\n",
      "2018-06-01 23:07:37,329 | INFO : ces\n",
      "2018-06-01 23:07:41,079 | INFO : deu\n",
      "2018-06-01 23:07:43,115 | INFO : por\n",
      "2018-06-01 23:07:49,748 | INFO : sme\n",
      "2018-06-01 23:07:52,042 | INFO : hin\n",
      "2018-06-01 23:07:54,674 | INFO : swe\n",
      "2018-06-01 23:07:54,839 | INFO : ina\n",
      "2018-06-01 23:07:56,438 | INFO : hbs\n",
      "2018-06-01 23:07:57,785 | INFO : tat\n",
      "2018-06-01 23:07:58,892 | INFO : eus\n",
      "2018-06-01 23:08:00,237 | INFO : nld\n",
      "2018-06-01 23:08:02,053 | INFO : slv\n",
      "2018-06-01 23:08:04,039 | INFO : ron\n",
      "2018-06-01 23:08:04,767 | INFO : bul\n",
      "2018-06-01 23:08:10,962 | INFO : nor\n",
      "2018-06-01 23:08:11,746 | INFO : isl\n",
      "2018-06-01 23:08:14,270 | INFO : mkd\n",
      "2018-06-01 23:08:15,594 | INFO : bre\n",
      "2018-06-01 23:08:18,042 | INFO : glg\n",
      "2018-06-01 23:08:20,313 | INFO : dan\n",
      "2018-06-01 23:08:24,010 | INFO : gle\n",
      "2018-06-01 23:08:24,375 | INFO : mlt\n",
      "2018-06-01 23:08:25,302 | INFO : hun\n",
      "2018-06-01 23:08:25,908 | INFO : mar\n",
      "2018-06-01 23:08:26,528 | INFO : kir\n",
      "2018-06-01 23:08:33,644 | INFO : nob\n",
      "2018-06-01 23:08:33,913 | INFO : heb\n",
      "2018-06-01 23:08:34,118 | INFO : asm\n",
      "2018-06-01 23:08:34,364 | INFO : ben\n",
      "2018-06-01 23:08:35,084 | INFO : cym\n",
      "2018-06-01 23:08:35,190 | INFO : ell\n",
      "2018-06-01 23:08:35,449 | INFO : cos\n",
      "2018-06-01 23:08:36,022 | INFO : slk\n",
      "2018-06-01 23:08:36,500 | INFO : chv\n",
      "2018-06-01 23:08:36,525 | INFO : lit\n",
      "2018-06-01 23:08:37,469 | INFO : pes\n",
      "2018-06-01 23:08:38,002 | INFO : fao\n",
      "2018-06-01 23:08:38,608 | INFO : est\n",
      "2018-06-01 23:08:38,825 | INFO : udm\n",
      "2018-06-01 23:08:38,925 | INFO : uzb\n",
      "2018-06-01 23:08:39,430 | INFO : kpv\n",
      "2018-06-01 23:08:39,528 | INFO : lat\n",
      "2018-06-01 23:08:39,535 | INFO : lav\n",
      "2018-06-01 23:08:41,103 | INFO : oci\n",
      "2018-06-01 23:08:41,488 | INFO : afr\n",
      "2018-06-01 23:08:41,842 | INFO : ara\n",
      "2018-06-01 23:08:43,237 | INFO : arg\n",
      "2018-06-01 23:08:45,139 | INFO : bel\n",
      "2018-06-01 23:08:45,206 | INFO : khk\n",
      "2018-06-01 23:08:47,144 | INFO : srd\n",
      "2018-06-01 23:08:47,366 | INFO : sqi\n",
      "2018-06-01 23:08:47,379 | INFO : tel\n",
      "2018-06-01 23:08:47,632 | INFO : nep\n",
      "2018-06-01 23:08:47,647 | INFO : krl\n",
      "2018-06-01 23:08:47,665 | INFO : olo\n",
      "2018-06-01 23:08:47,835 | INFO : kaa\n",
      "2018-06-01 23:08:47,873 | INFO : sah\n",
      "2018-06-01 23:08:48,115 | INFO : uig\n",
      "2018-06-01 23:08:48,208 | INFO : myv\n",
      "2018-06-01 23:08:48,796 | INFO : ukr\n",
      "2018-06-01 23:08:52,167 | INFO : sma\n",
      "2018-06-01 23:08:55,716 | INFO : smj\n",
      "2018-06-01 23:08:55,871 | INFO : snd\n",
      "2018-06-01 23:08:55,883 | INFO : swa\n",
      "2018-06-01 23:08:57,316 | INFO : tha\n",
      "2018-06-01 23:08:57,500 | INFO : urd\n",
      "2018-06-01 23:08:57,509 | INFO : zul\n",
      "2018-06-01 23:08:57,656 | INFO : ava\n",
      "2018-06-01 23:08:57,681 | INFO : bua\n",
      "2018-06-01 23:08:57,745 | INFO : byv\n",
      "2018-06-01 23:08:57,826 | INFO : ckb\n",
      "2018-06-01 23:08:58,095 | INFO : crh\n",
      "2018-06-01 23:08:58,238 | INFO : ltz\n",
      "2018-06-01 23:08:58,309 | INFO : lvs\n",
      "2018-06-01 23:08:58,611 | INFO : sco\n",
      "2018-06-01 23:08:58,753 | INFO : srn\n",
      "2018-06-01 23:08:58,763 | INFO : fas\n",
      "2018-06-01 23:08:58,953 | INFO : eu_bis\n",
      "2018-06-01 23:08:59,071 | INFO : fkv\n",
      "2018-06-01 23:08:59,361 | INFO : gla\n",
      "2018-06-01 23:09:00,066 | INFO : glv\n",
      "2018-06-01 23:09:00,093 | INFO : grn\n",
      "2018-06-01 23:09:00,131 | INFO : guc\n",
      "2018-06-01 23:09:00,139 | INFO : guj\n",
      "2018-06-01 23:09:00,208 | INFO : hat\n",
      "2018-06-01 23:09:00,228 | INFO : haw\n",
      "2018-06-01 23:09:00,246 | INFO : hbs_HR\n",
      "2018-06-01 23:09:00,265 | INFO : hbs_SR\n",
      "2018-06-01 23:09:00,609 | INFO : pan\n",
      "2018-06-01 23:09:01,005 | INFO : hye\n",
      "2018-06-01 23:09:01,311 | INFO : ind\n",
      "2018-06-01 23:09:01,798 | INFO : msa\n",
      "2018-06-01 23:09:01,816 | INFO : kan\n",
      "2018-06-01 23:09:01,838 | INFO : kum\n",
      "2018-06-01 23:09:01,849 | INFO : tyv\n",
      "2018-06-01 23:09:02,317 | INFO : kmr\n",
      "2018-06-01 23:09:02,332 | INFO : kom\n",
      "2018-06-01 23:09:02,342 | INFO : mhr\n",
      "2018-06-01 23:09:02,346 | INFO : lug\n",
      "2018-06-01 23:09:02,349 | INFO : liv\n",
      "2018-06-01 23:09:02,550 | INFO : mal\n",
      "2018-06-01 23:09:02,571 | INFO : mfe\n",
      "2018-06-01 23:09:02,582 | INFO : mrj\n",
      "2018-06-01 23:09:02,674 | INFO : mdf\n",
      "2018-06-01 23:09:02,688 | INFO : fry\n",
      "2018-06-01 23:09:04,912 | INFO : nno\n",
      "2018-06-01 23:09:04,964 | INFO : nog\n",
      "2018-06-01 23:09:04,969 | INFO : glk\n",
      "2018-06-01 23:09:05,002 | INFO : csb\n",
      "2018-06-01 23:09:05,025 | INFO : dsb\n",
      "2018-06-01 23:09:05,030 | INFO : hsb\n",
      "2018-06-01 23:09:05,067 | INFO : quz\n",
      "2018-06-01 23:09:05,089 | INFO : rup\n",
      "2018-06-01 23:09:05,519 | INFO : scn\n",
      "2018-06-01 23:09:05,531 | INFO : sin\n",
      "2018-06-01 23:09:05,536 | INFO : sjo\n",
      "2018-06-01 23:09:07,332 | INFO : smn\n",
      "2018-06-01 23:09:09,066 | INFO : ast\n",
      "2018-06-01 23:09:09,252 | INFO : qve\n",
      "2018-06-01 23:09:09,357 | INFO : ssp\n",
      "2018-06-01 23:09:09,367 | INFO : run\n",
      "2018-06-01 23:09:09,474 | INFO : bak\n",
      "2018-06-01 23:09:09,572 | INFO : tet\n",
      "2018-06-01 23:09:09,735 | INFO : tgk\n",
      "2018-06-01 23:09:09,746 | INFO : tgl\n",
      "2018-06-01 23:09:09,750 | INFO : ceb\n",
      "2018-06-01 23:09:09,783 | INFO : lao\n",
      "2018-06-01 23:09:09,788 | INFO : tlh\n",
      "2018-06-01 23:09:09,908 | INFO : tuk\n",
      "2018-06-01 23:09:10,115 | INFO : aze\n",
      "2018-06-01 23:09:10,131 | INFO : vie\n",
      "2018-06-01 23:09:10,140 | INFO : vro\n",
      "2018-06-01 23:09:10,586 | INFO : zho\n",
      "2018-06-01 23:09:10,623 | INFO : zh_CN\n",
      "2018-06-01 23:09:10,739 | INFO : zh_TW\n",
      "2018-06-01 23:09:10,746 | INFO : ssw\n",
      "2018-06-01 23:09:10,750 | INFO : xho\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%time monodix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'n'.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiGetItem:\n",
    "    def __init__(self):\n",
    "        self.list = []\n",
    "        self.dict = {}\n",
    "    \n",
    "    def add(self, word):\n",
    "        if len (word.s) > 1:\n",
    "            self.list.append(word)\n",
    "        else:\n",
    "            self.dict[word] = word\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        key2 = Word(key.lemma, key.lang, [''])\n",
    "        if key in self.dict:\n",
    "            return self.dict[key]\n",
    "        else:\n",
    "            if key2 in self.dict:\n",
    "                return self.dict[key2]\n",
    "            try:\n",
    "                key = self.list[self.list.index(key)]\n",
    "                return key\n",
    "            except:\n",
    "                print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[],[1]]\n",
    "key = a[a.index([])]\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_mono(lang):\n",
    "    dictionary = DiGetItem()\n",
    "    with open ('./monodix/{}.dix'.format(lang), 'r', encoding='utf-16') as f:\n",
    "        for line in f:\n",
    "            string = line.strip('\\n').split('\\t')\n",
    "            s = [Tags([j for j in i.split('-') if j !='']) for i in string[1].strip().split('$')]\n",
    "            dictionary.add(Word(string[0], lang, s))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epo = import_mono('ukr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DiGetItem at 0x1a508fb3978>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in epo.dict:\n",
    "    if i.s == [['adj']]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if '':\n",
    "    print ('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'' in Tags([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in epo.dict:\n",
    "    if ['adj','sint'] in i.s:\n",
    "        print (list(epo.dict[i].s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_word(word, lang):\n",
    "    s = word.findall('.//s')\n",
    "    s = [i.attrib['n'] for i in s]\n",
    "    if word.text: st = str(word.text)\n",
    "    else: st = ''\n",
    "    #s = Tags([i for i in s if i != ''])\n",
    "    s = Tags(s)\n",
    "    if '_' in st:\n",
    "        st = st.replace('_',' ')\n",
    "    return Word(st, lang, s)\n",
    "\n",
    "def parse_bidix (tree, l1, l2):\n",
    "    tree = tree.find('section')\n",
    "    if not tree:\n",
    "        pass\n",
    "        #print (l1, l2)\n",
    "    else:\n",
    "        for e in tree:\n",
    "            if 'n' in e.attrib:\n",
    "                side = e.attrib['n']\n",
    "            else:\n",
    "                side = ''\n",
    "            p = e.find('p')\n",
    "            if p:\n",
    "                yield one_word(p.find('l'), l1), one_word(p.find('r'), l2), side\n",
    "            else:\n",
    "                i = e.find('i')\n",
    "                if i:\n",
    "                    yield one_word(i, l1), one_word(i, l2), side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check (word1, word2, lang1, lang2):\n",
    "    #word1 = lang1[lang1.index(word1)]\n",
    "    #word2 = lang2[lang2.index(word2)]\n",
    "    word1 = lang1[word1]\n",
    "    word2 = lang2[word2]\n",
    "    return word1, word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def existance(pair, nodes):\n",
    "    if pair[0] in nodes and pair[1] in nodes:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def load_file(l1, l2):\n",
    "    with open ('language_list.csv','r',encoding='utf-8') as f:\n",
    "        languages = set([i.split('\\t')[1].strip() for i in f.readlines()])\n",
    "    with open ('{}-{}'.format(l1, l2), 'w', encoding='utf-16') as f:\n",
    "        for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "            for fl in files:\n",
    "                pair = fl.replace('.dix','').split('-')\n",
    "                if existance(pair, languages):\n",
    "                    logging.info('{}-{} started'.format(pair[0], pair[1]))\n",
    "                    lang1 = import_mono(pair[0])\n",
    "                    lang2 = import_mono(pair[1])\n",
    "                    with open (root+fl, 'r', encoding='utf-8') as d:\n",
    "                        try:\n",
    "                            tree = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))\n",
    "                            for word1, word2, side in parse_bidix (tree, pair[0], pair[1]):\n",
    "                                try:\n",
    "                                    word1, word2 = check (word1, word2, lang1, lang2)\n",
    "                                    string = str(side) + '\\t' + word1.write(mode='bi') + '\\t' + word2.write(mode='bi') + '\\n'\n",
    "                                    f.write(string)\n",
    "                                except:\n",
    "                                    pass\n",
    "                        except:\n",
    "                            print ('ERROR: {}-{}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'eng$$-'\n",
    "s = s.split('$')[-1]\n",
    "s = [i for i in s.split('-') if i !='']\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-'.join(['',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng = import_mono('eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng$general high school$[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word('general high school','eng',[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng$general high school$[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng[Word('general high school','eng',[])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "afr = import_mono('afr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "afr$self$[prn_prn-ref]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afr[Word('self','afr',['prn'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = Word('ryksgebied','afr',['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return:  afr$ryksgebied$[n]\n"
     ]
    }
   ],
   "source": [
    "key2 = Word(key.lemma, key.lang, [''])\n",
    "if key in afr.dict:\n",
    "    print ('return: ', afr.dict[key])\n",
    "else:\n",
    "    if key2 in afr.dict:\n",
    "        print ('return: ', afr.dict[key2])\n",
    "    try:\n",
    "        key = afr.list[afr.list.index(key)]\n",
    "        print ('return: ', key)\n",
    "    except:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-01 23:09:18,466 | INFO : afr-nld started\n",
      "....\n",
      "2018-06-01 23:10:01,228 | INFO : bre-cym started\n",
      "cym$ffôn$[n-]\n",
      "...\n",
      "2018-06-01 23:12:14,963 | INFO : ces-ces started\n",
      "ces$abê$[cnjsub]\n",
      "<--- CES ERRORS BECAUSE OF SAME left-right language --->ces$národnosť$[n-f]\n",
      "ces$čislo$[n-nt]\n",
      "...\n",
      "2018-06-01 23:30:02,333 | INFO : epo-bul started\n",
      "ERROR: epo-bul\n",
      "...\n",
      "2018-06-01 23:32:45,249 | INFO : epo-fas started\n",
      "ERROR: epo-fas\n",
      "...\n",
      "2018-06-01 23:33:46,875 | INFO : epo-pol started\n",
      "ERROR: epo-pol\n",
      "...\n",
      "2018-06-01 23:38:06,601 | INFO : fin-deu started\n",
      "fin$asuin$[n-compound-only-L]\n",
      "fin$uudelleen$[n-compound-only-L]\n",
      "fin$uudelleen$[n-compound-only-L]\n",
      "fin$lyhyt$[adj-pos-compound-only-L]\n",
      "deu$Kurz$[atp-cmp-split]\n",
      "2018-06-01 23:38:12,484 | INFO : fin-eng started\n",
      "2018-06-01 23:39:37,890 | INFO : fin-fra started\n",
      "ERROR: fin-fra\n",
      "...\n",
      "2018-06-02 00:01:07,068 | INFO : pol-lav started\n",
      "ERROR: pol-lav\n",
      "...\n",
      "2018-06-02 00:03:14,444 | INFO : sah-eng started\n",
      "ERROR: sah-eng\n",
      "...\n",
      "2018-06-02 00:16:50,957 | INFO : vie-eng started\n",
      "Wall time: 1h 7min 35s\n"
     ]
    }
   ],
   "source": [
    "%time load_file('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = './dictionaries/afr-nld.dix'\n",
    "pair = ['afr','nld']\n",
    "with open ('language_list.csv','r',encoding='utf-8') as f:\n",
    "        languages = set([i.split('\\t')[1].strip() for i in f.readlines()])\n",
    "if existance(pair, languages):\n",
    "    logging.info('{}-{} started'.format(pair[0], pair[1]))\n",
    "    lang1 = import_mono(pair[0])\n",
    "    lang2 = import_mono(pair[1])\n",
    "    with open (fl, 'r', encoding='utf-8') as d:\n",
    "        try:\n",
    "            tree = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))\n",
    "            for word1, word2, side in parse_bidix (tree, pair[0], pair[1]):\n",
    "                try:\n",
    "                    word1, word2 = check (word1, word2, lang1, lang2)\n",
    "                    string = str(side) + '\\t' + word1.write(mode='bi') + '\\t' + word2.write(mode='bi') + '\\n'\n",
    "                    #f.write(string)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            print ('ERROR: {}-{}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_encoding(file):\n",
    "    with open(file, 'r', encoding='utf-16') as f:\n",
    "        text = f.read()\n",
    "    text = text.encode('utf-8')\n",
    "    text = text.decode('utf-8')\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_encoding('rus-fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    side, lang1, lemma1, tags1, lang2, lemma2, tags2 = line.strip('\\n').split('\\t')\n",
    "    tags1 = [Tags(i.split('-')) for i in tags1.split('$')]\n",
    "    tags2 = [Tags(i.split('-')) for i in tags2.split('$')]\n",
    "    return side, Word(lemma1, lang1, tags1), Word(lemma2, lang2, tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_from_file(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield parse_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_from_file(file):\n",
    "    G = nx.DiGraph()\n",
    "    for side, word1, word2 in nodes_from_file(file):\n",
    "        if not side:\n",
    "            G.add_edge(word1, word2)\n",
    "            G.add_edge(word2, word1)\n",
    "        elif side == 'LR':\n",
    "            G.add_edge(word1, word2)\n",
    "        elif side == 'RL':\n",
    "            G.add_edgr(word2, word1)\n",
    "        else:\n",
    "            print (side)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search (changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetWithFilter(set):\n",
    "    def lemma(self, value):\n",
    "        return set(i for i in self if i.lemma == value)\n",
    "    def lang(self, value):\n",
    "        return set(i for i in self if i.lang == value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionaries(lang1,lang2):\n",
    "    l1 = import_mono(lang1)\n",
    "    l2 = import_mono(lang2)\n",
    "    l1 = SetWithFilter(l1.list+list(l1.dict.keys()))\n",
    "    l2 = SetWithFilter(l2.list+list(l2.dict.keys()))\n",
    "    return l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_search (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    for word in lemmas:\n",
    "        print(word, end='\\t')\n",
    "        s = SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        s = s.lang(l2)\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        for translation in s:\n",
    "            t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results, n=7):\n",
    "    for i in results:\n",
    "        print ('\\n\\t\\t', i)\n",
    "        for j in sorted(results[i], key=results[i].get, reverse=True)[:n]:\n",
    "            print (j, results[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%time l1, l2 = dictionaries('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$кот$[n-m-aa]\tall:  53\tfiltered:  8\n",
      "\n",
      "\t\t rus$кот$[n-m-aa]\n",
      "fra$chat$[n-GD] 1.2389736067509398\n",
      "fra$chat$[n-f] 1.2389736067509398\n",
      "fra$chat mâle$[n-m] 0.36787944117144233\n",
      "fra$matou$[n-m] 0.36787944117144233\n",
      "fra$chat$[n-m_n_n-m-ND] 0.17196656101408103\n",
      "fra$salon$[n-m_n_n-m-ND_n-m-sg] 0.01865110151663669\n",
      "fra$bavardage$[n-m_n_n-m-ND] 0.018321783101087508\n",
      "Wall time: 72.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time print_results(lemma_search (G, 'кот', l1, 'fra', 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full RUS-FRA (on many languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_languages(l1, l2):\n",
    "    G = nx.Graph()\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix', '').split('-')\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    pair = [l1, l2]\n",
    "    \n",
    "    with open('languages','r', encoding='utf-8') as f:\n",
    "        languages = f.read().split('\\t')\n",
    "        \n",
    "    with open('language_list.csv','w', encoding='utf-8') as f:\n",
    "        nodes = set()\n",
    "        for i in range(1,5):\n",
    "            w = nx.single_source_shortest_path_length(G, pair[0], cutoff=i)\n",
    "            v = nx.single_source_shortest_path_length(G, pair[1], cutoff=i)\n",
    "            H = G.subgraph(w.keys())\n",
    "            H.remove_node(pair[0])\n",
    "            H2 = G.subgraph(v.keys())\n",
    "            H2.remove_node(pair[1])\n",
    "            if pair[1] in H.nodes():\n",
    "                v = nx.node_connected_component(H, pair[1])\n",
    "            else:\n",
    "                v = set()\n",
    "            if pair[0] in H2.nodes():\n",
    "                w = nx.node_connected_component(H, pair[1])\n",
    "            else:\n",
    "                w = set() \n",
    "            nodes2 = v & w | set([pair[0], pair[1]])\n",
    "            nodes2 = nodes2 - nodes\n",
    "            #for node in nodes2:\n",
    "            #    f.write('{}\\t{}\\n'.format(i*2, node))\n",
    "            for lang in languages:\n",
    "                if lang in nodes2:\n",
    "                    f.write('{}\\t{}\\n'.format(i*2, lang))\n",
    "            nodes = nodes | nodes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_languages('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_search (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    for word in lemmas:\n",
    "        print(word, end='\\t')\n",
    "        #%time SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        s = SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        #%time s.lang(l2)\n",
    "        s = s.lang(l2)\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        for translation in s:\n",
    "            #%time t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import islice\n",
    "#def k_shortest_paths(G, source, target, k, weight=None):\n",
    "#    return list(islice(nx.shortest_simple_paths(G, source, target, weight=weight), k))\n",
    "\n",
    "class FilteredList(list):\n",
    "    def lemma(self, value):\n",
    "        return list(i for i in self if i.lemma == value)\n",
    "    def lang(self, value):\n",
    "        return list(i for i in self if i.lang == value)\n",
    "\n",
    "def lemma_search2 (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    for word in lemmas:\n",
    "        print(word, end='\\t')\n",
    "        #%time SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        s = FilteredList(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        #%time s.lang(l2)\n",
    "        s = s.lang(l2)[:20]\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        for translation in s:\n",
    "            #%time t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = nx.all_simple_paths(G, word, translation, cutoff=cutoff)\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Loading file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%time G = built_from_file('rus-fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861933 4164055\n"
     ]
    }
   ],
   "source": [
    "print (len(G.nodes()), len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$книга$[n-f-nn_n_n-f]\n",
      "bul$книга$[n-f]\n",
      "mkd$книга$[n-f_n]\n",
      "kpv$книга$[n]\n",
      "ukr$книга$[n-f_n-f-nn]\n",
      "udm$книга$[n]\n"
     ]
    }
   ],
   "source": [
    "for i in G.nodes():\n",
    "    if i.lemma == 'книга':\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%time l1, l2 = dictionaries('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$собака$[n-f-aa_n_n-f]\tall:  3327\tfiltered:  20\n",
      "\n",
      "\t\t rus$собака$[n-f-aa_n_n-f]\n",
      "fra$arobace$[n-f] 0.7357588823428847\n",
      "fra$escargot$[n-m_n] 0.42440445653839176\n",
      "fra$étau$[n-m_n] 0.4176665095393063\n",
      "fra$chien$[n-GD] 0.374617429569905\n",
      "fra$limaçon$[n-m] 0.3746173881705278\n",
      "fra$singe$[n-m_n] 0.3746173881705278\n",
      "fra$but$[n-m_n_n-m-ND] 0.3703581933481087\n",
      "fra$arobase$[n-f] 0.36787944117144233\n",
      "fra$loge$[n-f_n] 0.36787944117144233\n",
      "fra$goal$[n_n-m] 0.36787944117144233\n",
      "Wall time: 649 ms\n"
     ]
    }
   ],
   "source": [
    "%time print_results(lemma_search (G, 'собака', l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$город$[n-m-nn_n-m_n]\tall:  4173\tfiltered:  49\n",
      "\n",
      "\t\t rus$город$[n-m-nn_n-m_n]\n",
      "fra$municipalité$[n_n-f] 0.3861950800601765\n",
      "fra$mégalopole$[n-f] 0.36821490379934485\n",
      "fra$commune$[n-f_n-f-ND] 0.36800285097552904\n",
      "fra$commander$[vblex] 0.36787944117144233\n",
      "fra$implantation$[n-f_n] 0.36787944117144233\n",
      "fra$province$[n_n-f_n-f-ND] 0.36787944117144233\n",
      "fra$convier$[vblex] 0.36787944117144233\n",
      "fra$château$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$demander$[vblex] 0.36787944117144233\n",
      "fra$centre$[n-m_n_n-m-ND] 0.36787944117144233\n",
      "Wall time: 1.92 s\n",
      "rus$город$[n-m-nn_n-m_n]\tall:  4173\tfiltered:  20\n",
      "\n",
      "\t\t rus$город$[n-m-nn_n-m_n]\n",
      "fra$municipalité$[n_n-f] 0.3861950800601765\n",
      "fra$mégalopole$[n-f] 0.36821490379934485\n",
      "fra$commune$[n-f_n-f-ND] 0.36800285097552904\n",
      "fra$conseil municipal$[n-m_n] 0.36787944117144233\n",
      "fra$cité$[n_n-f_n-f-ND] 0.13533532463598988\n",
      "fra$ville$[n-f_n_n-f-ND] 0.1353352835155595\n",
      "fra$poste$[n-m_n-m-ND_n] 0.1353352832366127\n",
      "fra$capital$[n-m_n_n-m-ND] 0.1353352832366127\n",
      "fra$bourgade$[n_n-f_n-f-ND] 0.1353352832366127\n",
      "fra$grande ville$[n-f] 0.1353352832366127\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "word = 'город'\n",
    "%time print_results(lemma_search (G, word, l1, 'fra', 4), 10)\n",
    "%time print_results(lemma_search2 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$кошка$[n-f-aa]\tall:  1564\tfiltered:  16\n",
      "\n",
      "\t\t rus$кошка$[n-f-aa]\n",
      "fra$chat$[n-GD] 0.3863184898642632\n",
      "fra$chat$[n-f] 0.3861973403895835\n",
      "fra$jeunesse$[n-f_n_n-f-ND] 0.36787944117144233\n",
      "fra$fête$[n-f_n_n-f-ND] 0.36787944117144233\n",
      "fra$parti$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$match$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$chaton$[n-m] 0.36787944117144233\n",
      "fra$chien$[n-GD] 0.36787944117144233\n",
      "fra$chien$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$gaillard$[n_n-m_n-m-ND] 0.1353352832366127\n",
      "Wall time: 314 ms\n",
      "rus$кошка$[n-f-aa]\tall:  1564\tfiltered:  16\n",
      "\n",
      "\t\t rus$кошка$[n-f-aa]\n",
      "fra$chat$[n-GD] 0.3863184898642632\n",
      "fra$chat$[n-f] 0.3861973403895835\n",
      "fra$chaton$[n-m] 0.36787944117144233\n",
      "fra$fête$[n-f_n_n-f-ND] 0.36787944117144233\n",
      "fra$match$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$parti$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$chien$[n-GD] 0.36787944117144233\n",
      "fra$chien$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$jeunesse$[n-f_n_n-f-ND] 0.36787944117144233\n",
      "fra$partenaire$[n-mf_n_n-mf-ND] 0.1353352832366127\n",
      "Wall time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'кошка'\n",
    "%time print_results(lemma_search (G, word, l1, 'fra', 4), 10)\n",
    "%time print_results(lemma_search2 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_search3 (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    k = 0\n",
    "    for word in lemmas:\n",
    "        print(word)\n",
    "        s = FilteredList(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        if len(s) > 100:\n",
    "            print ('all: ', str(len(s)), end='\\t')\n",
    "            s = s.lang(l2)\n",
    "            print ('filtered: ', str(len(s)))\n",
    "            if len(s) > 10:\n",
    "                s = s[:20]\n",
    "        else:\n",
    "            print ('all: ', str(len(s)), end='\\t')\n",
    "            s = s.lang(l2)\n",
    "            print ('filtered: ', str(len(s)))\n",
    "            k = 1\n",
    "            if len(s) > 10:\n",
    "                s = s[:20]\n",
    "            else:\n",
    "                while k < 8:\n",
    "                    s = FilteredList(nx.single_source_shortest_path_length(G, word, cutoff=cutoff+k))\n",
    "                    print ('all: ', str(len(s)), end='\\t')\n",
    "                    s = s.lang(l2)\n",
    "                    print ('filtered: ', str(len(s)))\n",
    "                    if len(s) > 5:\n",
    "                        s = s[:20]\n",
    "                        break\n",
    "                    else:\n",
    "                        k += 1\n",
    "        for translation in s:\n",
    "            #%time t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = nx.all_simple_paths(G, word, translation, cutoff=cutoff+k)\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$простокваша$[n-f-nn]\n",
      "all:  14\tfiltered:  1\n",
      "all:  16\tfiltered:  1\n",
      "all:  20\tfiltered:  1\n",
      "all:  24\tfiltered:  1\n",
      "all:  77\tfiltered:  2\n",
      "all:  148\tfiltered:  2\n",
      "all:  1109\tfiltered:  22\n",
      "\n",
      "\t\t rus$простокваша$[n-f-nn]\n",
      "fra$kéfir$[n_n-m_n-m-sg] 1.00642944881611\n",
      "fra$lait$[n-m_n_n-m-ND] 0.1353353957717874\n",
      "fra$chaussure$[n-f_n_n-f-ND] 0.1353352832366127\n",
      "fra$soulier$[n-m_n] 0.1353352832366127\n",
      "fra$gorgée$[n-f_n-f-ND] 0.1353352832366127\n",
      "fra$lion$[n-GD] 0.1353352832366127\n",
      "fra$lion$[n_n-m_n-m-ND] 0.1353352832366127\n",
      "fra$soupape$[n_n-f] 0.1353352832366127\n",
      "fra$valve$[n_n-f] 0.1353352832366127\n",
      "fra$ton$[n_n-m_n-m-ND] 0.1353352832366127\n",
      "Wall time: 370 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'простокваша'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$ряженка$[n-f-nn]\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "\n",
      "\t\t rus$ряженка$[n-f-nn]\n",
      "Wall time: 37 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'ряженка'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$просо$[n-nt-nn_n]\n",
      "all:  50\tfiltered:  2\n",
      "all:  131\tfiltered:  4\n",
      "all:  246\tfiltered:  6\n",
      "\n",
      "\t\t rus$просо$[n-nt-nn_n]\n",
      "fra$millet$[n-m] 0.8781675752064854\n",
      "fra$panique$[n_n-f] 0.3746173881705278\n",
      "fra$étape$[n_n-f_n-f-ND] 0.1353352832366127\n",
      "fra$mille$[n-m] 0.01831563916768099\n",
      "fra$mile$[n-m] 0.01831563888873418\n",
      "fra$panique$[n-m_n-m-ND] 0.006740207328492448\n",
      "Wall time: 184 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'просо'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$гречка$[n-f-nn]\n",
      "all:  36\tfiltered:  3\n",
      "all:  39\tfiltered:  3\n",
      "all:  41\tfiltered:  4\n",
      "all:  48\tfiltered:  4\n",
      "all:  85\tfiltered:  6\n",
      "\n",
      "\t\t rus$гречка$[n-f-nn]\n",
      "fra$blé sarrasin$[n-m] 0.7947626498865004\n",
      "fra$blé noir$[n-m] 0.7947626498865004\n",
      "fra$froment$[n_n-m_n-m-ND] 0.7424968293419701\n",
      "fra$maïs$[n-m] 0.36787944117144233\n",
      "fra$sarrasin$[n-m] 0.33968515569537805\n",
      "fra$blé$[n_n-m_n-m-ND] 0.0009118819655545162\n",
      "Wall time: 150 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'гречка'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$собака$[n-f-aa_n_n-f]\n",
      "all:  3327\tfiltered:  51\n",
      "\n",
      "\t\t rus$собака$[n-f-aa_n_n-f]\n",
      "fra$arobace$[n-f] 0.7357588823428847\n",
      "fra$escargot$[n-m_n] 0.42440445653839176\n",
      "fra$étau$[n-m_n] 0.4176665095393063\n",
      "fra$chien$[n-GD] 0.374617429569905\n",
      "fra$limaçon$[n-m] 0.3746173881705278\n",
      "fra$singe$[n-m_n] 0.3746173881705278\n",
      "fra$but$[n-m_n_n-m-ND] 0.3703581933481087\n",
      "fra$arobase$[n-f] 0.36787944117144233\n",
      "fra$loge$[n-f_n] 0.36787944117144233\n",
      "fra$goal$[n_n-m] 0.36787944117144233\n",
      "Wall time: 711 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'собака'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time list(islice(range(1, 1000000000000000000000000000000000000000000000000000000000000000000), 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
