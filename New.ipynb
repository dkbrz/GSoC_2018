{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('1', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(Word('José','q',['1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q_José_[1]'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('1', 'r', encoding='utf-8') as f:\n",
    "    s = f.read()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enc (word):\n",
    "    s = word.encode('utf-8')\n",
    "    s = s.decode('utf-8')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, lemma, lang, s=[]):\n",
    "        if lemma == None: self.lemma = ''\n",
    "        else: self.lemma = enc(lemma)\n",
    "        self.lang = lang\n",
    "        self.s = s\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.s:\n",
    "            if isinstance(self.s[0],list):\n",
    "                w = '['+'_'.join(['-'.join(i) for i in self.s])+']'\n",
    "            else:\n",
    "                w = '['+'-'.join(self.s)+']'\n",
    "        else:\n",
    "            w = '-'\n",
    "        return str(self.lang)+'_'+str(self.lemma)+'_'+str(w)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.lemma == other.lemma and self.lang == other.lang and (self.s == other.s or other.s in self.s or self.s in other.s)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if self.lang == other.lang:\n",
    "            if self.lemma == other.lemma:\n",
    "                s1 = set(self.s)\n",
    "                s2 = set(other.s)\n",
    "                if (not s1 - s2) and (s1&s2==s1) and (s2 - s1):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "    \n",
    "    def write(self, mode='mono'):\n",
    "        if mode == 'mono':\n",
    "            return self.lemma + '\\t' + '_'.join([str(i) for i in self.s])\n",
    "        elif mode == 'bi':\n",
    "            return self.lang + '\\t' +  self.lemma + '\\t' + '_'.join([str(i) for i in self.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_languages():\n",
    "    G = nx.Graph()\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix', '').split('-')\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    d = G.degree()\n",
    "    d = sorted(d, key=d.get, reverse=True)\n",
    "    #print (d)\n",
    "    with open('languages','w',encoding='utf-8') as f:\n",
    "        f.write('\\t'.join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "%time all_languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monodix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tags(list):\n",
    "    def __le__(self, other):\n",
    "        s1 = set(self)\n",
    "        s2 = set(other)\n",
    "        if not s1 - s2 and s1&s2==s1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        s1 = set(self)\n",
    "        s2 = set(other)\n",
    "        if (not s1 - s2) and (s1&s2==s1) and (s2 - s1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if set(self) == set(other):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '-'.join(self)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordDict(dict):\n",
    "    def lemma(self, lemma):\n",
    "        self.lemma = lemma\n",
    "        \n",
    "class FilteredDict(dict):\n",
    "    def set_lang(self, lang):\n",
    "        self.lang = lang\n",
    "    \n",
    "    def lemma(self, lemma):\n",
    "        return self[self.lang+'_'+lemma]\n",
    "        \n",
    "    def add(self, word):\n",
    "        lemma = word.lang+'_'+word.lemma\n",
    "        tags = Tags(word.s)\n",
    "        if lemma in self:\n",
    "            #pass\n",
    "            if tags in self[lemma]:\n",
    "                self[lemma][tags] += 1\n",
    "            else:\n",
    "                self[lemma][tags] = 1\n",
    "        else:\n",
    "            self[lemma] = WordDict()\n",
    "            self[lemma].lemma(lemma)\n",
    "            self[lemma][tags] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_language_dict(lang):\n",
    "    dictionary = FilteredDict()\n",
    "    dictionary.set_lang(lang)\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix','').split('-')\n",
    "            if lang in pair:\n",
    "                if lang == pair[0]: side = 'l'\n",
    "                else: side = 'r'\n",
    "                try:\n",
    "                    with open (root+fl, 'r', encoding='utf-8') as d:\n",
    "                        t = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))     \n",
    "                    for word in parse_one(t, side, lang):\n",
    "                        #try:\n",
    "                        #    #print (word)\n",
    "                        dictionary.add(word)\n",
    "                        #except:\n",
    "                        #    print (word)\n",
    "                except:\n",
    "                    pass\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_word(word, lang):\n",
    "    s = word.findall('.//s')\n",
    "    s = [i.attrib['n'] for i in s]\n",
    "    if word.text: st = str(word.text)\n",
    "    else: st = ''\n",
    "    return Word(st, lang, s)\n",
    "\n",
    "def parse_one (tree, side, lang):\n",
    "    tree = tree.find('section')\n",
    "    #print (len(tree))\n",
    "    for e in tree:\n",
    "        p = e.find('p')\n",
    "        if p:\n",
    "            word = one_word(p.find(side), lang)\n",
    "            yield word\n",
    "        else:\n",
    "            i = e.find('i')\n",
    "            #print (i)\n",
    "            if i:\n",
    "                word = one_word(i, lang)\n",
    "                yield word\n",
    "            else:\n",
    "                pass\n",
    "            #print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shorten(word_dict, f):\n",
    "    short = []\n",
    "    for i in sorted(word_dict, key=lambda x: (word_dict[x], -len(x)), reverse=True):\n",
    "        t = True\n",
    "        for key, j in enumerate(short):\n",
    "            if (j[0] < i) or (i < j[0]):\n",
    "                short[key].append(i)\n",
    "                t = False\n",
    "                break\n",
    "        if t:\n",
    "            short.append([i])\n",
    "    f.write(word_dict.lemma)\n",
    "    f.write(word_dict.lemma[4:])\n",
    "    word = word_dict.lemma[4:]\n",
    "    return word, short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dictionary_to_nodes(dictionary):\n",
    "    with open ('1', 'w', encoding = 'utf-8') as f:\n",
    "        for i in dictionary.keys():\n",
    "            word, tags = shorten(dictionary[i], f)\n",
    "            f.write(word+'\\n')\n",
    "            #print (word)\n",
    "            if '_' in word:\n",
    "                word = word.replace('_', ' ')\n",
    "            for tag in tags:\n",
    "                f.write(str(Word(word,'1',['']))+'\\n')\n",
    "                f.write(str(Word(word, dictionary.lang, Tags([i for i in tag if i])))+'\\n')\n",
    "                yield Word(word, dictionary.lang, Tags([i for i in tag if i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def monodix():\n",
    "    if not os.path.exists('./monodix/'):\n",
    "        os.makedirs('./monodix/')\n",
    "    with open('languages','r', encoding='utf-8') as f:\n",
    "        langs = f.read().split('\\t')\n",
    "    for lang in langs:\n",
    "        #print (lang, end = '\\t')\n",
    "        dictionary = one_language_dict(lang)\n",
    "        #with open ('1', 'w', encoding = 'utf-8') as f:\n",
    "        #    for i in dictionary:\n",
    "        #        f.write(i+'\\n')\n",
    "        with open ('./monodix/'+lang+'.dix', 'w', encoding = 'utf-16') as f:\n",
    "            #f.write('José')\n",
    "            for i in dictionary_to_nodes(dictionary):\n",
    "                f.write (i.write(mode='mono')+'\\n')\n",
    "        logging.info(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-31 20:37:02,535 | INFO : eng\n",
      "2018-05-31 20:37:15,077 | INFO : spa\n",
      "2018-05-31 20:37:22,967 | INFO : fin\n",
      "2018-05-31 20:37:34,538 | INFO : epo\n",
      "2018-05-31 20:37:41,414 | INFO : rus\n",
      "2018-05-31 20:37:48,238 | INFO : ita\n",
      "2018-05-31 20:37:55,538 | INFO : fra\n",
      "2018-05-31 20:37:58,382 | INFO : pol\n",
      "2018-05-31 20:38:08,821 | INFO : cat\n",
      "2018-05-31 20:38:12,289 | INFO : kaz\n",
      "2018-05-31 20:38:14,272 | INFO : tur\n",
      "2018-05-31 20:38:15,810 | INFO : ces\n",
      "2018-05-31 20:38:20,463 | INFO : deu\n",
      "2018-05-31 20:38:22,678 | INFO : por\n",
      "2018-05-31 20:38:30,726 | INFO : sme\n",
      "2018-05-31 20:38:33,662 | INFO : hin\n",
      "2018-05-31 20:38:36,828 | INFO : swe\n",
      "2018-05-31 20:38:37,033 | INFO : ina\n",
      "2018-05-31 20:38:38,963 | INFO : hbs\n",
      "2018-05-31 20:38:40,554 | INFO : tat\n",
      "2018-05-31 20:38:41,757 | INFO : eus\n",
      "2018-05-31 20:38:43,646 | INFO : nld\n",
      "2018-05-31 20:38:45,976 | INFO : slv\n",
      "2018-05-31 20:38:48,092 | INFO : ron\n",
      "2018-05-31 20:38:49,040 | INFO : bul\n",
      "2018-05-31 20:38:56,022 | INFO : nor\n",
      "2018-05-31 20:38:56,904 | INFO : isl\n",
      "2018-05-31 20:38:59,983 | INFO : mkd\n",
      "2018-05-31 20:39:01,749 | INFO : bre\n",
      "2018-05-31 20:39:04,506 | INFO : glg\n",
      "2018-05-31 20:39:07,364 | INFO : dan\n",
      "2018-05-31 20:39:11,456 | INFO : gle\n",
      "2018-05-31 20:39:12,189 | INFO : mlt\n",
      "2018-05-31 20:39:13,366 | INFO : hun\n",
      "2018-05-31 20:39:14,076 | INFO : mar\n",
      "2018-05-31 20:39:14,624 | INFO : kir\n",
      "2018-05-31 20:39:23,022 | INFO : nob\n",
      "2018-05-31 20:39:23,350 | INFO : heb\n",
      "2018-05-31 20:39:23,518 | INFO : asm\n",
      "2018-05-31 20:39:23,869 | INFO : ben\n",
      "2018-05-31 20:39:24,710 | INFO : cym\n",
      "2018-05-31 20:39:24,851 | INFO : ell\n",
      "2018-05-31 20:39:25,074 | INFO : cos\n",
      "2018-05-31 20:39:25,725 | INFO : slk\n",
      "2018-05-31 20:39:26,188 | INFO : chv\n",
      "2018-05-31 20:39:26,249 | INFO : lit\n",
      "2018-05-31 20:39:27,414 | INFO : pes\n",
      "2018-05-31 20:39:28,115 | INFO : fao\n",
      "2018-05-31 20:39:29,037 | INFO : est\n",
      "2018-05-31 20:39:29,252 | INFO : udm\n",
      "2018-05-31 20:39:29,416 | INFO : uzb\n",
      "2018-05-31 20:39:30,005 | INFO : kpv\n",
      "2018-05-31 20:39:30,094 | INFO : lat\n",
      "2018-05-31 20:39:30,108 | INFO : lav\n",
      "2018-05-31 20:39:31,756 | INFO : oci\n",
      "2018-05-31 20:39:32,288 | INFO : afr\n",
      "2018-05-31 20:39:32,747 | INFO : ara\n",
      "2018-05-31 20:39:34,413 | INFO : arg\n",
      "2018-05-31 20:39:36,770 | INFO : bel\n",
      "2018-05-31 20:39:36,871 | INFO : khk\n",
      "2018-05-31 20:39:39,303 | INFO : srd\n",
      "2018-05-31 20:39:39,638 | INFO : sqi\n",
      "2018-05-31 20:39:39,650 | INFO : tel\n",
      "2018-05-31 20:39:40,021 | INFO : nep\n",
      "2018-05-31 20:39:40,054 | INFO : krl\n",
      "2018-05-31 20:39:40,079 | INFO : olo\n",
      "2018-05-31 20:39:40,287 | INFO : kaa\n",
      "2018-05-31 20:39:40,327 | INFO : sah\n",
      "2018-05-31 20:39:40,545 | INFO : uig\n",
      "2018-05-31 20:39:40,684 | INFO : myv\n",
      "2018-05-31 20:39:41,409 | INFO : ukr\n",
      "2018-05-31 20:39:45,920 | INFO : sma\n",
      "2018-05-31 20:39:50,145 | INFO : smj\n",
      "2018-05-31 20:39:50,365 | INFO : snd\n",
      "2018-05-31 20:39:50,387 | INFO : swa\n",
      "2018-05-31 20:39:52,023 | INFO : tha\n",
      "2018-05-31 20:39:52,272 | INFO : urd\n",
      "2018-05-31 20:39:52,326 | INFO : zul\n",
      "2018-05-31 20:39:52,630 | INFO : ava\n",
      "2018-05-31 20:39:52,660 | INFO : bua\n",
      "2018-05-31 20:39:52,738 | INFO : byv\n",
      "2018-05-31 20:39:52,806 | INFO : ckb\n",
      "2018-05-31 20:39:53,065 | INFO : crh\n",
      "2018-05-31 20:39:53,256 | INFO : ltz\n",
      "2018-05-31 20:39:53,432 | INFO : lvs\n",
      "2018-05-31 20:39:53,937 | INFO : sco\n",
      "2018-05-31 20:39:54,098 | INFO : srn\n",
      "2018-05-31 20:39:54,107 | INFO : fas\n",
      "2018-05-31 20:39:54,219 | INFO : eu_bis\n",
      "2018-05-31 20:39:54,390 | INFO : fkv\n",
      "2018-05-31 20:39:54,826 | INFO : gla\n",
      "2018-05-31 20:39:55,555 | INFO : glv\n",
      "2018-05-31 20:39:55,592 | INFO : grn\n",
      "2018-05-31 20:39:55,637 | INFO : guc\n",
      "2018-05-31 20:39:55,648 | INFO : guj\n",
      "2018-05-31 20:39:55,721 | INFO : hat\n",
      "2018-05-31 20:39:55,767 | INFO : haw\n",
      "2018-05-31 20:39:55,823 | INFO : hbs_HR\n",
      "2018-05-31 20:39:55,853 | INFO : hbs_SR\n",
      "2018-05-31 20:39:56,303 | INFO : pan\n",
      "2018-05-31 20:39:56,690 | INFO : hye\n",
      "2018-05-31 20:39:57,303 | INFO : ind\n",
      "2018-05-31 20:39:57,702 | INFO : msa\n",
      "2018-05-31 20:39:57,715 | INFO : kan\n",
      "2018-05-31 20:39:57,757 | INFO : kum\n",
      "2018-05-31 20:39:57,772 | INFO : tyv\n",
      "2018-05-31 20:39:58,366 | INFO : kmr\n",
      "2018-05-31 20:39:58,389 | INFO : kom\n",
      "2018-05-31 20:39:58,398 | INFO : mhr\n",
      "2018-05-31 20:39:58,405 | INFO : lug\n",
      "2018-05-31 20:39:58,411 | INFO : liv\n",
      "2018-05-31 20:39:58,645 | INFO : mal\n",
      "2018-05-31 20:39:58,661 | INFO : mfe\n",
      "2018-05-31 20:39:58,694 | INFO : mrj\n",
      "2018-05-31 20:39:58,866 | INFO : mdf\n",
      "2018-05-31 20:39:58,896 | INFO : fry\n",
      "2018-05-31 20:40:01,623 | INFO : nno\n",
      "2018-05-31 20:40:01,688 | INFO : nog\n",
      "2018-05-31 20:40:01,705 | INFO : glk\n",
      "2018-05-31 20:40:01,760 | INFO : csb\n",
      "2018-05-31 20:40:01,782 | INFO : dsb\n",
      "2018-05-31 20:40:01,790 | INFO : hsb\n",
      "2018-05-31 20:40:01,827 | INFO : quz\n",
      "2018-05-31 20:40:01,866 | INFO : rup\n",
      "2018-05-31 20:40:02,256 | INFO : scn\n",
      "2018-05-31 20:40:02,274 | INFO : sin\n",
      "2018-05-31 20:40:02,283 | INFO : sjo\n",
      "2018-05-31 20:40:04,569 | INFO : smn\n",
      "2018-05-31 20:40:06,638 | INFO : ast\n",
      "2018-05-31 20:40:06,861 | INFO : qve\n",
      "2018-05-31 20:40:06,999 | INFO : ssp\n",
      "2018-05-31 20:40:07,007 | INFO : run\n",
      "2018-05-31 20:40:07,134 | INFO : bak\n",
      "2018-05-31 20:40:07,382 | INFO : tet\n",
      "2018-05-31 20:40:07,500 | INFO : tgk\n",
      "2018-05-31 20:40:07,531 | INFO : tgl\n",
      "2018-05-31 20:40:07,539 | INFO : ceb\n",
      "2018-05-31 20:40:07,591 | INFO : lao\n",
      "2018-05-31 20:40:07,596 | INFO : tlh\n",
      "2018-05-31 20:40:07,734 | INFO : tuk\n",
      "2018-05-31 20:40:08,009 | INFO : aze\n",
      "2018-05-31 20:40:08,028 | INFO : vie\n",
      "2018-05-31 20:40:08,039 | INFO : vro\n",
      "2018-05-31 20:40:08,638 | INFO : zho\n",
      "2018-05-31 20:40:08,688 | INFO : zh_CN\n",
      "2018-05-31 20:40:08,741 | INFO : zh_TW\n",
      "2018-05-31 20:40:08,747 | INFO : ssw\n",
      "2018-05-31 20:40:08,752 | INFO : xho\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%time monodix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'n'.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_mono(lang):\n",
    "    dictionary = []\n",
    "    with open ('./monodix/{}.dix'.format(lang), 'r', encoding='utf-16') as f:\n",
    "        for line in f:\n",
    "            string = line.split('\\t')\n",
    "            dictionary.append(Word(string[0], lang, [Tags(i.split('-')) for i in string[1].strip().split('_')]))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_word(word, lang):\n",
    "    s = word.findall('.//s')\n",
    "    s = [i.attrib['n'] for i in s]\n",
    "    if word.text: st = str(word.text)\n",
    "    else: st = ''\n",
    "    #st = st.encode('utf-8')\n",
    "    #st = st.decode('utf-8')\n",
    "    #print(st)\n",
    "    return Word(st, lang, s)\n",
    "\n",
    "def parse_bidix (tree, l1, l2):\n",
    "    tree = tree.find('section')\n",
    "    if not tree:\n",
    "        pass\n",
    "        #print (l1, l2)\n",
    "    else:\n",
    "        for e in tree:\n",
    "            if 'n' in e.attrib:\n",
    "                side = e.attrib['n']\n",
    "            else:\n",
    "                side = ''\n",
    "            p = e.find('p')\n",
    "            if p:\n",
    "                yield one_word(p.find('l'), l1), one_word(p.find('r'), l2), side\n",
    "            else:\n",
    "                i = e.find('i')\n",
    "                if i:\n",
    "                    yield one_word(i, l1), one_word(i, l2), side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check (word1, word2, lang1, lang2):\n",
    "    word1 = lang1[lang1.index(word1)]\n",
    "    word2 = lang2[lang2.index(word2)]\n",
    "    return word1, word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def existance(pair, nodes):\n",
    "    if pair[0] in nodes and pair[1] in nodes:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def load_file(l1, l2):\n",
    "    with open ('language_list.csv','r',encoding='utf-8') as f:\n",
    "        languages = set([i.split('\\t')[1].strip() for i in f.readlines()])\n",
    "    with open ('{}-{}'.format(l1, l2), 'w', encoding='utf-8') as f:\n",
    "        for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "            for fl in files:\n",
    "                #print (fl)\n",
    "                pair = fl.replace('.dix','').split('-')\n",
    "                #print(pair)\n",
    "                if existance(pair, languages):\n",
    "                    logging.info('{}-{} started'.format(pair[0], pair[1]))\n",
    "                    lang1 = import_mono(pair[0])\n",
    "                    lang2 = import_mono(pair[1])\n",
    "                    with open (root+fl, 'r', encoding='utf-8') as d:\n",
    "                        try:\n",
    "                            tree = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))\n",
    "                            for word1, word2, side in parse_bidix (tree, pair[0], pair[1]):\n",
    "                                try:\n",
    "                                    word1, word2 = check (word1, word2, lang1, lang2)\n",
    "                                    string = str(side) + '\\t' + word1.write(mode='bi') + '\\t' + word2.write(mode='bi') + '\\n'\n",
    "                                    f.write(string)\n",
    "                                except:\n",
    "                                    if word1 not in lang1:\n",
    "                                        print ('\\t', word1, end='\\t')\n",
    "                                    elif word2 not in lang2:\n",
    "                                        print ('\\t', word2, end='\\t')\n",
    "                            print ()\n",
    "                        except:\n",
    "                            print ('ERROR: {}-{}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-31 22:18:54,426 | INFO : bel-epo started\n",
      "\n",
      "2018-05-31 22:19:03,068 | INFO : bel-rus started\n",
      "\n",
      "2018-05-31 22:29:56,036 | INFO : eng-deu started\n"
     ]
    }
   ],
   "source": [
    "%time load_file('rus', 'fra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [Word('f','f',[Tags(['n','adj']),Tags(['v'])])]\n",
    "Word('f','f',['v']) in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(1, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (isinstance(Word(1,1,Tags(['n','adj'])).lemma, str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './monodix/deu.dix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-24c9f7e22d1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'eng'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlang1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_mono\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlang2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_mono\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'./dictionaries/eng-deu.dix'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<b/>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<.?g>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-35a69297e21b>\u001b[0m in \u001b[0;36mimport_mono\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_mono\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'./monodix/{}.dix'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-16'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './monodix/deu.dix'"
     ]
    }
   ],
   "source": [
    "pair = ['eng', 'deu']\n",
    "lang1 = import_mono(pair[0])\n",
    "lang2 = import_mono(pair[1])\n",
    "with open ('./dictionaries/eng-deu.dix', 'r', encoding='utf-8') as d:\n",
    "    tree = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))\n",
    "    for word1, word2, side in parse_bidix (tree, pair[0], pair[1]):\n",
    "        word1, word2 = check (word1, word2, lang1, lang2)\n",
    "        #print (word1, word2)\n",
    "        string = str(side) + '\\t' + word1.write(mode='bi') + '\\t' + word2.write(mode='bi') + '\\n'\n",
    "        #print (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word ('José','eng',['np']) in lang1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%time lang1 = import_mono('eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang1[0] in lang1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = Word('а', 'bel', ['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word in lang1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'а'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang1[0].lemma = str(lang1[0].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang1[1].lemma == 'аазіс'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'а'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang1[0].lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а ﻿а\n"
     ]
    }
   ],
   "source": [
    "print (word.lemma, lang1[0].lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Tags"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lang1[0].s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"аб'яктыў\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang1[10].lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang1[0].s == word.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bel'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n] ['n']\n"
     ]
    }
   ],
   "source": [
    "print (lang1[0].lemma, word.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(lang1[0].s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'а'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-0df2c3d6465c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlang1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# == word.lemma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "lang1[0].lemma.decode('utf-8')# == word.lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word('а', 'bel', Tags(['n'])) in lang1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
