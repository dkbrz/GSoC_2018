{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc (word):\n",
    "    s = word.encode('utf-8')\n",
    "    s = s.decode('utf-8')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, lemma, lang, s=[]):\n",
    "        if lemma == None: self.lemma = ''\n",
    "        else: self.lemma = enc(lemma)\n",
    "        self.lang = lang\n",
    "        self.s = s\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.s:\n",
    "            if isinstance(self.s[0],list):\n",
    "                w = '['+'_'.join(['-'.join(i) for i in self.s])+']'\n",
    "            else:\n",
    "                w = '['+'-'.join(self.s)+']'\n",
    "        else:\n",
    "            w = '-'\n",
    "        return str(self.lang)+'$'+str(self.lemma)+'$'+str(w)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.lemma == other.lemma and self.lang == other.lang and (self.s == other.s or other.s in self.s or self.s in other.s)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if self.lang == other.lang:\n",
    "            if self.lemma == other.lemma:\n",
    "                s1 = set(self.s)\n",
    "                s2 = set(other.s)\n",
    "                if (not s1 - s2) and (s1&s2==s1) and (s2 - s1):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "    \n",
    "    def write(self, mode='mono'):\n",
    "        if mode == 'mono':\n",
    "            return self.lemma + '\\t' + '$'.join([str(i) for i in self.s])\n",
    "        elif mode == 'bi':\n",
    "            return self.lang + '\\t' +  self.lemma + '\\t' + '$'.join([str(i) for i in self.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_languages():\n",
    "    G = nx.Graph()\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix', '').split('-')\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    d = G.degree()\n",
    "    d = sorted(d, key=d.get, reverse=True)\n",
    "    #print (d)\n",
    "    with open('languages','w',encoding='utf-8') as f:\n",
    "        f.write('\\t'.join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "%time all_languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monodix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tags(list):\n",
    "    def __le__(self, other):\n",
    "        s1 = set(self)\n",
    "        s2 = set(other)\n",
    "        if not s1 - s2 and s1&s2==s1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        s1 = set(self)\n",
    "        s2 = set(other)\n",
    "        if (not s1 - s2) and (s1&s2==s1) and (s2 - s1):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if set(self) == set(other):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '-'.join(self)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDict(dict):\n",
    "    def lemma(self, lemma):\n",
    "        self.lemma = lemma\n",
    "        \n",
    "class FilteredDict(dict):\n",
    "    def set_lang(self, lang):\n",
    "        self.lang = lang\n",
    "    \n",
    "    def lemma(self, lemma):\n",
    "        return self[self.lang+'_'+lemma]\n",
    "        \n",
    "    def add(self, word):\n",
    "        lemma = word.lang+'_'+word.lemma\n",
    "        tags = Tags(word.s)\n",
    "        if lemma in self:\n",
    "            if tags in self[lemma]:\n",
    "                self[lemma][tags] += 1\n",
    "            else:\n",
    "                self[lemma][tags] = 1\n",
    "        else:\n",
    "            self[lemma] = WordDict()\n",
    "            self[lemma].lemma(lemma)\n",
    "            self[lemma][tags] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_language_dict(lang):\n",
    "    dictionary = FilteredDict()\n",
    "    dictionary.set_lang(lang)\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix','').split('-')\n",
    "            if lang in pair:\n",
    "                if lang == pair[0]: side = 'l'\n",
    "                else: side = 'r'\n",
    "                try:\n",
    "                    with open (root+fl, 'r', encoding='utf-8') as d:\n",
    "                        t = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))     \n",
    "                    for word in parse_one(t, side, lang):\n",
    "                        dictionary.add(word)\n",
    "                except:\n",
    "                    pass\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = one_language_dict('afr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{prn-tn-m: 1, prn-tn-mf: 2, det-ind: 1, prn: 1}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['afr_almal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('almal', [[prn-tn-mf, prn], [det-ind], [prn-tn-m]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten(dictionary['afr_almal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(word_dict):\n",
    "    short = []\n",
    "    for i in sorted(word_dict, key=lambda x: (word_dict[x], -len(x)), reverse=True):\n",
    "        new = True\n",
    "        for key, j in enumerate(short):\n",
    "            inner = True\n",
    "            for key2, k in enumerate(j):\n",
    "                if (k < i) or (i < k): pass\n",
    "                else: inner = False\n",
    "            if inner: \n",
    "                short[key].append(i)\n",
    "                new = False\n",
    "        if new: short.append([i])\n",
    "    word = word_dict.lemma[4:]\n",
    "    return word, short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_word(word, lang):\n",
    "    if word.text: st = str(word.text)\n",
    "    else: st = ''\n",
    "    s = [i.attrib['n'] for i in word.findall('.//s')]\n",
    "    s = [i for i in s if i != '']\n",
    "    return Word(st, lang, s)\n",
    "\n",
    "def parse_one (tree, side, lang):\n",
    "    tree = tree.find('section')\n",
    "    for e in tree:\n",
    "        p = e.find('p')\n",
    "        if p:\n",
    "            word = one_word(p.find(side), lang)\n",
    "            yield word\n",
    "        else:\n",
    "            i = e.find('i')\n",
    "            if i:\n",
    "                word = one_word(i, lang)\n",
    "                yield word\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_nodes(dictionary):\n",
    "    for i in dictionary.keys():\n",
    "        word, tags = shorten(dictionary[i])\n",
    "        if '_' in word:\n",
    "            word = word.replace('_', ' ')\n",
    "        for tag in tags:\n",
    "            yield Word(word, dictionary.lang, Tags([i for i in tag if i != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monodix():\n",
    "    if not os.path.exists('./monodix/'):\n",
    "        os.makedirs('./monodix/')\n",
    "    with open('languages','r', encoding='utf-8') as f:\n",
    "        langs = f.read().split('\\t')\n",
    "    for lang in langs:\n",
    "        dictionary = one_language_dict(lang)\n",
    "        with open ('./monodix/'+lang+'.dix', 'w', encoding = 'utf-16') as f:\n",
    "            for i in dictionary_to_nodes(dictionary):\n",
    "                f.write (i.write(mode='mono')+'\\n')\n",
    "        logging.info(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-07 18:11:53,831 | INFO : eng\n",
      "2018-06-07 18:12:07,021 | INFO : spa\n",
      "2018-06-07 18:12:14,277 | INFO : fin\n",
      "2018-06-07 18:12:23,971 | INFO : epo\n",
      "2018-06-07 18:12:30,287 | INFO : rus\n",
      "2018-06-07 18:12:36,600 | INFO : ita\n",
      "2018-06-07 18:12:43,339 | INFO : fra\n",
      "2018-06-07 18:12:45,995 | INFO : pol\n",
      "2018-06-07 18:12:55,739 | INFO : cat\n",
      "2018-06-07 18:12:58,650 | INFO : kaz\n",
      "2018-06-07 18:13:00,610 | INFO : tur\n",
      "2018-06-07 18:13:01,691 | INFO : ces\n",
      "2018-06-07 18:13:05,565 | INFO : deu\n",
      "2018-06-07 18:13:07,619 | INFO : por\n",
      "2018-06-07 18:13:14,566 | INFO : sme\n",
      "2018-06-07 18:13:17,014 | INFO : hin\n",
      "2018-06-07 18:13:19,833 | INFO : swe\n",
      "2018-06-07 18:13:20,039 | INFO : ina\n",
      "2018-06-07 18:13:21,746 | INFO : hbs\n",
      "2018-06-07 18:13:23,014 | INFO : tat\n",
      "2018-06-07 18:13:24,104 | INFO : eus\n",
      "2018-06-07 18:13:25,432 | INFO : nld\n",
      "2018-06-07 18:13:27,414 | INFO : slv\n",
      "2018-06-07 18:13:29,441 | INFO : ron\n",
      "2018-06-07 18:13:30,180 | INFO : bul\n",
      "2018-06-07 18:13:36,721 | INFO : nor\n",
      "2018-06-07 18:13:37,522 | INFO : isl\n",
      "2018-06-07 18:13:40,021 | INFO : mkd\n",
      "2018-06-07 18:13:41,408 | INFO : bre\n",
      "2018-06-07 18:13:43,936 | INFO : glg\n",
      "2018-06-07 18:13:46,256 | INFO : dan\n",
      "2018-06-07 18:13:50,219 | INFO : gle\n",
      "2018-06-07 18:13:50,688 | INFO : mlt\n",
      "2018-06-07 18:13:51,662 | INFO : hun\n",
      "2018-06-07 18:13:52,354 | INFO : mar\n",
      "2018-06-07 18:13:52,842 | INFO : kir\n",
      "2018-06-07 18:14:00,039 | INFO : nob\n",
      "2018-06-07 18:14:00,343 | INFO : heb\n",
      "2018-06-07 18:14:00,502 | INFO : asm\n",
      "2018-06-07 18:14:00,754 | INFO : ben\n",
      "2018-06-07 18:14:01,527 | INFO : cym\n",
      "2018-06-07 18:14:01,653 | INFO : ell\n",
      "2018-06-07 18:14:01,857 | INFO : cos\n",
      "2018-06-07 18:14:02,420 | INFO : slk\n",
      "2018-06-07 18:14:02,907 | INFO : chv\n",
      "2018-06-07 18:14:02,933 | INFO : lit\n",
      "2018-06-07 18:14:04,069 | INFO : pes\n",
      "2018-06-07 18:14:04,632 | INFO : fao\n",
      "2018-06-07 18:14:05,275 | INFO : est\n",
      "2018-06-07 18:14:05,449 | INFO : udm\n",
      "2018-06-07 18:14:05,538 | INFO : uzb\n",
      "2018-06-07 18:14:06,039 | INFO : kpv\n",
      "2018-06-07 18:14:06,147 | INFO : lat\n",
      "2018-06-07 18:14:06,153 | INFO : lav\n",
      "2018-06-07 18:14:07,705 | INFO : oci\n",
      "2018-06-07 18:14:08,092 | INFO : afr\n",
      "2018-06-07 18:14:08,456 | INFO : ara\n",
      "2018-06-07 18:14:09,806 | INFO : arg\n",
      "2018-06-07 18:14:11,781 | INFO : bel\n",
      "2018-06-07 18:14:11,862 | INFO : khk\n",
      "2018-06-07 18:14:13,835 | INFO : srd\n",
      "2018-06-07 18:14:13,986 | INFO : sqi\n",
      "2018-06-07 18:14:13,994 | INFO : tel\n",
      "2018-06-07 18:14:14,298 | INFO : nep\n",
      "2018-06-07 18:14:14,314 | INFO : krl\n",
      "2018-06-07 18:14:14,344 | INFO : olo\n",
      "2018-06-07 18:14:14,504 | INFO : kaa\n",
      "2018-06-07 18:14:14,520 | INFO : sah\n",
      "2018-06-07 18:14:14,700 | INFO : uig\n",
      "2018-06-07 18:14:14,821 | INFO : myv\n",
      "2018-06-07 18:14:15,372 | INFO : ukr\n",
      "2018-06-07 18:14:18,716 | INFO : sma\n",
      "2018-06-07 18:14:22,293 | INFO : smj\n",
      "2018-06-07 18:14:22,471 | INFO : snd\n",
      "2018-06-07 18:14:22,492 | INFO : swa\n",
      "2018-06-07 18:14:23,824 | INFO : tha\n",
      "2018-06-07 18:14:24,052 | INFO : urd\n",
      "2018-06-07 18:14:24,061 | INFO : zul\n",
      "2018-06-07 18:14:24,251 | INFO : ava\n",
      "2018-06-07 18:14:24,276 | INFO : bua\n",
      "2018-06-07 18:14:24,359 | INFO : byv\n",
      "2018-06-07 18:14:24,425 | INFO : ckb\n",
      "2018-06-07 18:14:24,648 | INFO : crh\n",
      "2018-06-07 18:14:24,768 | INFO : ltz\n",
      "2018-06-07 18:14:24,885 | INFO : lvs\n",
      "2018-06-07 18:14:25,201 | INFO : sco\n",
      "2018-06-07 18:14:25,353 | INFO : srn\n",
      "2018-06-07 18:14:25,368 | INFO : fas\n",
      "2018-06-07 18:14:25,549 | INFO : eu_bis\n",
      "2018-06-07 18:14:25,669 | INFO : fkv\n",
      "2018-06-07 18:14:25,952 | INFO : gla\n",
      "2018-06-07 18:14:26,664 | INFO : glv\n",
      "2018-06-07 18:14:26,685 | INFO : grn\n",
      "2018-06-07 18:14:26,723 | INFO : guc\n",
      "2018-06-07 18:14:26,731 | INFO : guj\n",
      "2018-06-07 18:14:26,796 | INFO : hat\n",
      "2018-06-07 18:14:26,819 | INFO : haw\n",
      "2018-06-07 18:14:26,824 | INFO : hbs_HR\n",
      "2018-06-07 18:14:26,828 | INFO : hbs_SR\n",
      "2018-06-07 18:14:27,089 | INFO : pan\n",
      "2018-06-07 18:14:27,479 | INFO : hye\n",
      "2018-06-07 18:14:27,884 | INFO : ind\n",
      "2018-06-07 18:14:28,254 | INFO : msa\n",
      "2018-06-07 18:14:28,278 | INFO : kan\n",
      "2018-06-07 18:14:28,297 | INFO : kum\n",
      "2018-06-07 18:14:28,308 | INFO : tyv\n",
      "2018-06-07 18:14:28,743 | INFO : kmr\n",
      "2018-06-07 18:14:28,765 | INFO : kom\n",
      "2018-06-07 18:14:28,773 | INFO : mhr\n",
      "2018-06-07 18:14:28,777 | INFO : lug\n",
      "2018-06-07 18:14:28,781 | INFO : liv\n",
      "2018-06-07 18:14:29,029 | INFO : mal\n",
      "2018-06-07 18:14:29,043 | INFO : mfe\n",
      "2018-06-07 18:14:29,054 | INFO : mrj\n",
      "2018-06-07 18:14:29,171 | INFO : mdf\n",
      "2018-06-07 18:14:29,204 | INFO : fry\n",
      "2018-06-07 18:14:31,177 | INFO : nno\n",
      "2018-06-07 18:14:31,243 | INFO : nog\n",
      "2018-06-07 18:14:31,250 | INFO : glk\n",
      "2018-06-07 18:14:31,282 | INFO : csb\n",
      "2018-06-07 18:14:31,302 | INFO : dsb\n",
      "2018-06-07 18:14:31,309 | INFO : hsb\n",
      "2018-06-07 18:14:31,340 | INFO : quz\n",
      "2018-06-07 18:14:31,357 | INFO : rup\n",
      "2018-06-07 18:14:31,695 | INFO : scn\n",
      "2018-06-07 18:14:31,736 | INFO : sin\n",
      "2018-06-07 18:14:31,743 | INFO : sjo\n",
      "2018-06-07 18:14:33,389 | INFO : smn\n",
      "2018-06-07 18:14:35,219 | INFO : ast\n",
      "2018-06-07 18:14:35,452 | INFO : qve\n",
      "2018-06-07 18:14:35,585 | INFO : ssp\n",
      "2018-06-07 18:14:35,594 | INFO : run\n",
      "2018-06-07 18:14:35,751 | INFO : bak\n",
      "2018-06-07 18:14:35,893 | INFO : tet\n",
      "2018-06-07 18:14:36,020 | INFO : tgk\n",
      "2018-06-07 18:14:36,040 | INFO : tgl\n",
      "2018-06-07 18:14:36,046 | INFO : ceb\n",
      "2018-06-07 18:14:36,087 | INFO : lao\n",
      "2018-06-07 18:14:36,092 | INFO : tlh\n",
      "2018-06-07 18:14:36,237 | INFO : tuk\n",
      "2018-06-07 18:14:36,480 | INFO : aze\n",
      "2018-06-07 18:14:36,499 | INFO : vie\n",
      "2018-06-07 18:14:36,509 | INFO : vro\n",
      "2018-06-07 18:14:36,916 | INFO : zho\n",
      "2018-06-07 18:14:36,943 | INFO : zh_CN\n",
      "2018-06-07 18:14:36,947 | INFO : zh_TW\n",
      "2018-06-07 18:14:36,951 | INFO : ssw\n",
      "2018-06-07 18:14:36,956 | INFO : xho\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%time monodix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'n'.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiGetItem:\n",
    "    def __init__(self):\n",
    "        self.list = []\n",
    "        self.dict = {}\n",
    "    \n",
    "    def add(self, word):\n",
    "        if len (word.s) > 1:\n",
    "            self.list.append(word)\n",
    "        else:\n",
    "            self.dict[word] = word\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        key2 = Word(key.lemma, key.lang, [''])\n",
    "        if key in self.dict:\n",
    "            return self.dict[key]\n",
    "        else:\n",
    "            if key2 in self.dict:\n",
    "                return self.dict[key2]\n",
    "            try:\n",
    "                key = self.list[self.list.index(key)]\n",
    "                return key\n",
    "            except:\n",
    "                print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[],[1]]\n",
    "key = a[a.index([])]\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_mono(lang):\n",
    "    dictionary = DiGetItem()\n",
    "    with open ('./monodix/{}.dix'.format(lang), 'r', encoding='utf-16') as f:\n",
    "        for line in f:\n",
    "            string = line.strip('\\n').split('\\t')\n",
    "            s = [Tags([j for j in i.split('-') if j !='']) for i in string[1].strip().split('$')]\n",
    "            dictionary.add(Word(string[0], lang, s))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epo = import_mono('ukr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DiGetItem at 0x1a508fb3978>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in epo.dict:\n",
    "    if i.s == [['adj']]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if '':\n",
    "    print ('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'' in Tags([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in epo.dict:\n",
    "    if ['adj','sint'] in i.s:\n",
    "        print (list(epo.dict[i].s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_word(word, lang):\n",
    "    s = word.findall('.//s')\n",
    "    s = [i.attrib['n'] for i in s]\n",
    "    if word.text: st = str(word.text)\n",
    "    else: st = ''\n",
    "    #s = Tags([i for i in s if i != ''])\n",
    "    s = Tags(s)\n",
    "    if '_' in st:\n",
    "        st = st.replace('_',' ')\n",
    "    return Word(st, lang, s)\n",
    "\n",
    "def parse_bidix (tree, l1, l2):\n",
    "    tree = tree.find('section')\n",
    "    if not tree:\n",
    "        pass\n",
    "        #print (l1, l2)\n",
    "    else:\n",
    "        for e in tree:\n",
    "            if 'n' in e.attrib:\n",
    "                side = e.attrib['n']\n",
    "            else:\n",
    "                side = ''\n",
    "            p = e.find('p')\n",
    "            if p:\n",
    "                yield one_word(p.find('l'), l1), one_word(p.find('r'), l2), side\n",
    "            else:\n",
    "                i = e.find('i')\n",
    "                if i:\n",
    "                    yield one_word(i, l1), one_word(i, l2), side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check (word1, word2, lang1, lang2):\n",
    "    #word1 = lang1[lang1.index(word1)]\n",
    "    #word2 = lang2[lang2.index(word2)]\n",
    "    word1 = lang1[word1]\n",
    "    word2 = lang2[word2]\n",
    "    return word1, word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def existance(pair, nodes):\n",
    "    if pair[0] in nodes and pair[1] in nodes:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def load_file(l1, l2):\n",
    "    with open ('language_list.csv','r',encoding='utf-8') as f:\n",
    "        languages = set([i.split('\\t')[1].strip() for i in f.readlines()])\n",
    "    with open ('{}-{}'.format(l1, l2), 'w', encoding='utf-16') as f:\n",
    "        for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "            for fl in files:\n",
    "                pair = fl.replace('.dix','').split('-')\n",
    "                if existance(pair, languages):\n",
    "                    logging.info('{}-{} started'.format(pair[0], pair[1]))\n",
    "                    lang1 = import_mono(pair[0])\n",
    "                    lang2 = import_mono(pair[1])\n",
    "                    with open (root+fl, 'r', encoding='utf-8') as d:\n",
    "                        try:\n",
    "                            tree = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))\n",
    "                            for word1, word2, side in parse_bidix (tree, pair[0], pair[1]):\n",
    "                                try:\n",
    "                                    word1, word2 = check (word1, word2, lang1, lang2)\n",
    "                                    string = str(side) + '\\t' + word1.write(mode='bi') + '\\t' + word2.write(mode='bi') + '\\n'\n",
    "                                    f.write(string)\n",
    "                                except:\n",
    "                                    pass\n",
    "                        except:\n",
    "                            print ('ERROR: {}-{}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'eng$$-'\n",
    "s = s.split('$')[-1]\n",
    "s = [i for i in s.split('-') if i !='']\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-'.join(['',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng = import_mono('eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng$general high school$[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word('general high school','eng',[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng$general high school$[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng[Word('general high school','eng',[])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "afr = import_mono('afr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "afr$self$[prn_prn-ref]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afr[Word('self','afr',['prn'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = Word('ryksgebied','afr',['n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return:  afr$ryksgebied$[n]\n"
     ]
    }
   ],
   "source": [
    "key2 = Word(key.lemma, key.lang, [''])\n",
    "if key in afr.dict:\n",
    "    print ('return: ', afr.dict[key])\n",
    "else:\n",
    "    if key2 in afr.dict:\n",
    "        print ('return: ', afr.dict[key2])\n",
    "    try:\n",
    "        key = afr.list[afr.list.index(key)]\n",
    "        print ('return: ', key)\n",
    "    except:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-01 23:09:18,466 | INFO : afr-nld started\n",
      "....\n",
      "2018-06-01 23:10:01,228 | INFO : bre-cym started\n",
      "cym$ffôn$[n-]\n",
      "...\n",
      "2018-06-01 23:12:14,963 | INFO : ces-ces started\n",
      "ces$abê$[cnjsub]\n",
      "<--- CES ERRORS BECAUSE OF SAME left-right language --->ces$národnosť$[n-f]\n",
      "ces$čislo$[n-nt]\n",
      "...\n",
      "2018-06-01 23:30:02,333 | INFO : epo-bul started\n",
      "ERROR: epo-bul\n",
      "...\n",
      "2018-06-01 23:32:45,249 | INFO : epo-fas started\n",
      "ERROR: epo-fas\n",
      "...\n",
      "2018-06-01 23:33:46,875 | INFO : epo-pol started\n",
      "ERROR: epo-pol\n",
      "...\n",
      "2018-06-01 23:38:06,601 | INFO : fin-deu started\n",
      "fin$asuin$[n-compound-only-L]\n",
      "fin$uudelleen$[n-compound-only-L]\n",
      "fin$uudelleen$[n-compound-only-L]\n",
      "fin$lyhyt$[adj-pos-compound-only-L]\n",
      "deu$Kurz$[atp-cmp-split]\n",
      "2018-06-01 23:38:12,484 | INFO : fin-eng started\n",
      "2018-06-01 23:39:37,890 | INFO : fin-fra started\n",
      "ERROR: fin-fra\n",
      "...\n",
      "2018-06-02 00:01:07,068 | INFO : pol-lav started\n",
      "ERROR: pol-lav\n",
      "...\n",
      "2018-06-02 00:03:14,444 | INFO : sah-eng started\n",
      "ERROR: sah-eng\n",
      "...\n",
      "2018-06-02 00:16:50,957 | INFO : vie-eng started\n",
      "Wall time: 1h 7min 35s\n"
     ]
    }
   ],
   "source": [
    "%time load_file('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = './dictionaries/afr-nld.dix'\n",
    "pair = ['afr','nld']\n",
    "with open ('language_list.csv','r',encoding='utf-8') as f:\n",
    "        languages = set([i.split('\\t')[1].strip() for i in f.readlines()])\n",
    "if existance(pair, languages):\n",
    "    logging.info('{}-{} started'.format(pair[0], pair[1]))\n",
    "    lang1 = import_mono(pair[0])\n",
    "    lang2 = import_mono(pair[1])\n",
    "    with open (fl, 'r', encoding='utf-8') as d:\n",
    "        try:\n",
    "            tree = ET.fromstring(d.read().replace('<b/>',' ').replace('<.?g>',''))\n",
    "            for word1, word2, side in parse_bidix (tree, pair[0], pair[1]):\n",
    "                try:\n",
    "                    word1, word2 = check (word1, word2, lang1, lang2)\n",
    "                    string = str(side) + '\\t' + word1.write(mode='bi') + '\\t' + word2.write(mode='bi') + '\\n'\n",
    "                    #f.write(string)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            print ('ERROR: {}-{}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_encoding(file):\n",
    "    with open(file, 'r', encoding='utf-16') as f:\n",
    "        text = f.read()\n",
    "    text = text.encode('utf-8')\n",
    "    text = text.decode('utf-8')\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_encoding('rus-fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    side, lang1, lemma1, tags1, lang2, lemma2, tags2 = line.strip('\\n').split('\\t')\n",
    "    tags1 = [Tags(i.split('-')) for i in tags1.split('$')]\n",
    "    tags2 = [Tags(i.split('-')) for i in tags2.split('$')]\n",
    "    return side, Word(lemma1, lang1, tags1), Word(lemma2, lang2, tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_from_file(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield parse_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_from_file(file):\n",
    "    G = nx.DiGraph()\n",
    "    for side, word1, word2 in nodes_from_file(file):\n",
    "        if not side:\n",
    "            G.add_edge(word1, word2)\n",
    "            G.add_edge(word2, word1)\n",
    "        elif side == 'LR':\n",
    "            G.add_edge(word1, word2)\n",
    "        elif side == 'RL':\n",
    "            G.add_edgr(word2, word1)\n",
    "        else:\n",
    "            print (side)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search (changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetWithFilter(set):\n",
    "    def lemma(self, value):\n",
    "        return set(i for i in self if i.lemma == value)\n",
    "    def lang(self, value):\n",
    "        return set(i for i in self if i.lang == value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionaries(lang1,lang2):\n",
    "    l1 = import_mono(lang1)\n",
    "    l2 = import_mono(lang2)\n",
    "    l1 = SetWithFilter(l1.list+list(l1.dict.keys()))\n",
    "    l2 = SetWithFilter(l2.list+list(l2.dict.keys()))\n",
    "    return l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_search (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    for word in lemmas:\n",
    "        print(word, end='\\t')\n",
    "        s = SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        s = s.lang(l2)\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        for translation in s:\n",
    "            t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results, n=7):\n",
    "    for i in results:\n",
    "        print ('\\n\\t\\t', i)\n",
    "        for j in sorted(results[i], key=results[i].get, reverse=True)[:n]:\n",
    "            print (j, results[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%time l1, l2 = dictionaries('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$кот$[n-m-aa]\tall:  53\tfiltered:  8\n",
      "\n",
      "\t\t rus$кот$[n-m-aa]\n",
      "fra$chat$[n-GD] 1.2389736067509398\n",
      "fra$chat$[n-f] 1.2389736067509398\n",
      "fra$chat mâle$[n-m] 0.36787944117144233\n",
      "fra$matou$[n-m] 0.36787944117144233\n",
      "fra$chat$[n-m_n_n-m-ND] 0.17196656101408103\n",
      "fra$salon$[n-m_n_n-m-ND_n-m-sg] 0.01865110151663669\n",
      "fra$bavardage$[n-m_n_n-m-ND] 0.018321783101087508\n",
      "Wall time: 72.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time print_results(lemma_search (G, 'кот', l1, 'fra', 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full RUS-FRA (on many languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_languages(l1, l2):\n",
    "    G = nx.Graph()\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix', '').split('-')\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    pair = [l1, l2]\n",
    "    \n",
    "    with open('languages','r', encoding='utf-8') as f:\n",
    "        languages = f.read().split('\\t')\n",
    "        \n",
    "    with open('language_list.csv','w', encoding='utf-8') as f:\n",
    "        nodes = set()\n",
    "        for i in range(1,5):\n",
    "            w = nx.single_source_shortest_path_length(G, pair[0], cutoff=i)\n",
    "            v = nx.single_source_shortest_path_length(G, pair[1], cutoff=i)\n",
    "            H = G.subgraph(w.keys()).copy()\n",
    "            H.remove_node(pair[0])\n",
    "            H2 = G.subgraph(v.keys()).copy()\n",
    "            H2.remove_node(pair[1])\n",
    "            if pair[1] in H.nodes():\n",
    "                v = nx.node_connected_component(H, pair[1])\n",
    "            else:\n",
    "                v = set()\n",
    "            if pair[0] in H2.nodes():\n",
    "                w = nx.node_connected_component(H, pair[1])\n",
    "            else:\n",
    "                w = set() \n",
    "            nodes2 = v & w | set([pair[0], pair[1]])\n",
    "            nodes2 = nodes2 - nodes\n",
    "            #for node in nodes2:\n",
    "            #    f.write('{}\\t{}\\n'.format(i*2, node))\n",
    "            for lang in languages:\n",
    "                if lang in nodes2:\n",
    "                    f.write('{}\\t{}\\n'.format(i*2, lang))\n",
    "            nodes = nodes | nodes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_languages('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_search (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    for word in lemmas:\n",
    "        print(word, end='\\t')\n",
    "        #%time SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        s = SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        #%time s.lang(l2)\n",
    "        s = s.lang(l2)\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        for translation in s:\n",
    "            #%time t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import islice\n",
    "#def k_shortest_paths(G, source, target, k, weight=None):\n",
    "#    return list(islice(nx.shortest_simple_paths(G, source, target, weight=weight), k))\n",
    "\n",
    "class FilteredList(list):\n",
    "    def lemma(self, value):\n",
    "        return list(i for i in self if i.lemma == value)\n",
    "    def lang(self, value):\n",
    "        return list(i for i in self if i.lang == value)\n",
    "\n",
    "def lemma_search2 (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    for word in lemmas:\n",
    "        print(word, end='\\t')\n",
    "        #%time SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        s = FilteredList(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        #%time s.lang(l2)\n",
    "        s = s.lang(l2)[:20]\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        for translation in s:\n",
    "            #%time t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = nx.all_simple_paths(G, word, translation, cutoff=cutoff)\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Loading file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%time G = built_from_file('rus-fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861933 4164055\n"
     ]
    }
   ],
   "source": [
    "print (len(G.nodes()), len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$книга$[n-f-nn_n_n-f]\n",
      "bul$книга$[n-f]\n",
      "mkd$книга$[n-f_n]\n",
      "kpv$книга$[n]\n",
      "ukr$книга$[n-f_n-f-nn]\n",
      "udm$книга$[n]\n"
     ]
    }
   ],
   "source": [
    "for i in G.nodes():\n",
    "    if i.lemma == 'книга':\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%time l1, l2 = dictionaries('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$собака$[n-f-aa_n_n-f]\tall:  3327\tfiltered:  20\n",
      "\n",
      "\t\t rus$собака$[n-f-aa_n_n-f]\n",
      "fra$arobace$[n-f] 0.7357588823428847\n",
      "fra$escargot$[n-m_n] 0.42440445653839176\n",
      "fra$étau$[n-m_n] 0.4176665095393063\n",
      "fra$chien$[n-GD] 0.374617429569905\n",
      "fra$limaçon$[n-m] 0.3746173881705278\n",
      "fra$singe$[n-m_n] 0.3746173881705278\n",
      "fra$but$[n-m_n_n-m-ND] 0.3703581933481087\n",
      "fra$arobase$[n-f] 0.36787944117144233\n",
      "fra$loge$[n-f_n] 0.36787944117144233\n",
      "fra$goal$[n_n-m] 0.36787944117144233\n",
      "Wall time: 649 ms\n"
     ]
    }
   ],
   "source": [
    "%time print_results(lemma_search (G, 'собака', l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$город$[n-m-nn_n-m_n]\tall:  4173\tfiltered:  49\n",
      "\n",
      "\t\t rus$город$[n-m-nn_n-m_n]\n",
      "fra$municipalité$[n_n-f] 0.3861950800601765\n",
      "fra$mégalopole$[n-f] 0.36821490379934485\n",
      "fra$commune$[n-f_n-f-ND] 0.36800285097552904\n",
      "fra$commander$[vblex] 0.36787944117144233\n",
      "fra$implantation$[n-f_n] 0.36787944117144233\n",
      "fra$province$[n_n-f_n-f-ND] 0.36787944117144233\n",
      "fra$convier$[vblex] 0.36787944117144233\n",
      "fra$château$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$demander$[vblex] 0.36787944117144233\n",
      "fra$centre$[n-m_n_n-m-ND] 0.36787944117144233\n",
      "Wall time: 1.92 s\n",
      "rus$город$[n-m-nn_n-m_n]\tall:  4173\tfiltered:  20\n",
      "\n",
      "\t\t rus$город$[n-m-nn_n-m_n]\n",
      "fra$municipalité$[n_n-f] 0.3861950800601765\n",
      "fra$mégalopole$[n-f] 0.36821490379934485\n",
      "fra$commune$[n-f_n-f-ND] 0.36800285097552904\n",
      "fra$conseil municipal$[n-m_n] 0.36787944117144233\n",
      "fra$cité$[n_n-f_n-f-ND] 0.13533532463598988\n",
      "fra$ville$[n-f_n_n-f-ND] 0.1353352835155595\n",
      "fra$poste$[n-m_n-m-ND_n] 0.1353352832366127\n",
      "fra$capital$[n-m_n_n-m-ND] 0.1353352832366127\n",
      "fra$bourgade$[n_n-f_n-f-ND] 0.1353352832366127\n",
      "fra$grande ville$[n-f] 0.1353352832366127\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "word = 'город'\n",
    "%time print_results(lemma_search (G, word, l1, 'fra', 4), 10)\n",
    "%time print_results(lemma_search2 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$кошка$[n-f-aa]\tall:  1564\tfiltered:  16\n",
      "\n",
      "\t\t rus$кошка$[n-f-aa]\n",
      "fra$chat$[n-GD] 0.3863184898642632\n",
      "fra$chat$[n-f] 0.3861973403895835\n",
      "fra$jeunesse$[n-f_n_n-f-ND] 0.36787944117144233\n",
      "fra$fête$[n-f_n_n-f-ND] 0.36787944117144233\n",
      "fra$parti$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$match$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$chaton$[n-m] 0.36787944117144233\n",
      "fra$chien$[n-GD] 0.36787944117144233\n",
      "fra$chien$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$gaillard$[n_n-m_n-m-ND] 0.1353352832366127\n",
      "Wall time: 314 ms\n",
      "rus$кошка$[n-f-aa]\tall:  1564\tfiltered:  16\n",
      "\n",
      "\t\t rus$кошка$[n-f-aa]\n",
      "fra$chat$[n-GD] 0.3863184898642632\n",
      "fra$chat$[n-f] 0.3861973403895835\n",
      "fra$chaton$[n-m] 0.36787944117144233\n",
      "fra$fête$[n-f_n_n-f-ND] 0.36787944117144233\n",
      "fra$match$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$parti$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$chien$[n-GD] 0.36787944117144233\n",
      "fra$chien$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$jeunesse$[n-f_n_n-f-ND] 0.36787944117144233\n",
      "fra$partenaire$[n-mf_n_n-mf-ND] 0.1353352832366127\n",
      "Wall time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'кошка'\n",
    "%time print_results(lemma_search (G, word, l1, 'fra', 4), 10)\n",
    "%time print_results(lemma_search2 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translations(G, word, cutoff):\n",
    "    s = None\n",
    "    if word not in G.nodes():\n",
    "        return None\n",
    "    s = FilteredList(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "    if len(s) > 100:\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        s = s.lang(l2)\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        if len(s) > 10:\n",
    "            s = s[:20]\n",
    "    else:\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        s = s.lang(l2)\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        k = 1\n",
    "        if len(s) > 10:\n",
    "            s = s[:20]\n",
    "        else:\n",
    "            while k < 8:\n",
    "                s = FilteredList(nx.single_source_shortest_path_length(G, word, cutoff=cutoff+k))\n",
    "                print ('all: ', str(len(s)), end='\\t')\n",
    "                s = s.lang(l2)\n",
    "                print ('filtered: ', str(len(s)))\n",
    "                if len(s) > 5:\n",
    "                    s = s[:20]\n",
    "                    break\n",
    "                else:\n",
    "                    k += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_search3 (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    k = 0\n",
    "    for word in lemmas:\n",
    "        print(word)\n",
    "        if word not in G.nodes():\n",
    "            break\n",
    "        s = FilteredList(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        if len(s) > 100:\n",
    "            print ('all: ', str(len(s)), end='\\t')\n",
    "            s = s.lang(l2)\n",
    "            print ('filtered: ', str(len(s)))\n",
    "            if len(s) > 10:\n",
    "                s = s[:20]\n",
    "        else:\n",
    "            print ('all: ', str(len(s)), end='\\t')\n",
    "            s = s.lang(l2)\n",
    "            print ('filtered: ', str(len(s)))\n",
    "            k = 1\n",
    "            if len(s) > 10:\n",
    "                s = s[:20]\n",
    "            else:\n",
    "                while k < 8:\n",
    "                    s = FilteredList(nx.single_source_shortest_path_length(G, word, cutoff=cutoff+k))\n",
    "                    print ('all: ', str(len(s)), end='\\t')\n",
    "                    s = s.lang(l2)\n",
    "                    print ('filtered: ', str(len(s)))\n",
    "                    if len(s) > 5:\n",
    "                        s = s[:20]\n",
    "                        break\n",
    "                    else:\n",
    "                        k += 1\n",
    "        for translation in s:\n",
    "            #%time t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = nx.all_simple_paths(G, word, translation, cutoff=cutoff+k)\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$простокваша$[n-f-nn]\n",
      "all:  14\tfiltered:  1\n",
      "all:  16\tfiltered:  1\n",
      "all:  20\tfiltered:  1\n",
      "all:  24\tfiltered:  1\n",
      "all:  77\tfiltered:  2\n",
      "all:  148\tfiltered:  2\n",
      "all:  1109\tfiltered:  22\n",
      "\n",
      "\t\t rus$простокваша$[n-f-nn]\n",
      "fra$kéfir$[n_n-m_n-m-sg] 1.00642944881611\n",
      "fra$lait$[n-m_n_n-m-ND] 0.1353353957717874\n",
      "fra$chaussure$[n-f_n_n-f-ND] 0.1353352832366127\n",
      "fra$soulier$[n-m_n] 0.1353352832366127\n",
      "fra$gorgée$[n-f_n-f-ND] 0.1353352832366127\n",
      "fra$lion$[n-GD] 0.1353352832366127\n",
      "fra$lion$[n_n-m_n-m-ND] 0.1353352832366127\n",
      "fra$soupape$[n_n-f] 0.1353352832366127\n",
      "fra$valve$[n_n-f] 0.1353352832366127\n",
      "fra$ton$[n_n-m_n-m-ND] 0.1353352832366127\n",
      "Wall time: 629 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'простокваша'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$ряженка$[n-f-nn]\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "all:  2\tfiltered:  0\n",
      "\n",
      "\t\t rus$ряженка$[n-f-nn]\n",
      "Wall time: 46 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'ряженка'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$просо$[n-nt-nn_n]\n",
      "all:  50\tfiltered:  2\n",
      "all:  131\tfiltered:  4\n",
      "all:  246\tfiltered:  6\n",
      "\n",
      "\t\t rus$просо$[n-nt-nn_n]\n",
      "fra$millet$[n-m] 0.8781675752064854\n",
      "fra$panique$[n_n-f] 0.3746173881705278\n",
      "fra$étape$[n_n-f_n-f-ND] 0.1353352832366127\n",
      "fra$mille$[n-m] 0.01831563916768099\n",
      "fra$mile$[n-m] 0.01831563888873418\n",
      "fra$panique$[n-m_n-m-ND] 0.006740207328492448\n",
      "Wall time: 354 ms\n"
     ]
    }
   ],
   "source": [
    "word = 'просо'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$гречка$[n-f-nn]\n",
      "all:  36\tfiltered:  3\n",
      "all:  39\tfiltered:  3\n",
      "all:  41\tfiltered:  4\n",
      "all:  48\tfiltered:  4\n",
      "all:  85\tfiltered:  6\n",
      "\n",
      "\t\t rus$гречка$[n-f-nn]\n",
      "fra$blé sarrasin$[n-m] 0.7947626498865004\n",
      "fra$blé noir$[n-m] 0.7947626498865004\n",
      "fra$froment$[n_n-m_n-m-ND] 0.7424968293419701\n",
      "fra$maïs$[n-m] 0.36787944117144233\n",
      "fra$sarrasin$[n-m] 0.33968515569537805\n",
      "fra$blé$[n_n-m_n-m-ND] 0.0009118819655545162\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "word = 'гречка'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$страна$[n-f-nn_n-f_n]\n",
      "all:  8266\tfiltered:  115\n",
      "rus$страна$[np-top-f]\n",
      "\n",
      "\t\t rus$страна$[n-f-nn_n-f_n]\n",
      "fra$latéral$[n-f] 0.7357588823428847\n",
      "fra$latéral$[n-m] 0.503214724408055\n",
      "fra$habitant$[n-f] 0.3861950800601765\n",
      "fra$campagne$[n-f_n_n-f-ND] 0.3703581933481087\n",
      "fra$localité$[n-f_n_n-f-ND] 0.3703581933481087\n",
      "fra$page$[n-f_n_n-f-ND] 0.36879132313699686\n",
      "fra$bourgade$[n_n-f_n-f-ND] 0.36879132313699686\n",
      "fra$côté$[n-m_n_n-m-ND] 0.36821490379934485\n",
      "fra$masse$[n_n-f_n-f-ND] 0.36821490379934485\n",
      "fra$habitant$[n-m_n_n-m-pl] 0.3679248411012048\n",
      "\n",
      "\t\t rus$страна$[np-top-f]\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "word = 'страна'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$очень$[n]\n",
      "all:  43\tfiltered:  3\n",
      "all:  231\tfiltered:  7\n",
      "rus$очень$[adv]\n",
      "all:  2399\tfiltered:  72\n",
      "\n",
      "\t\t rus$очень$[n]\n",
      "fra$solde$[n_n-m_n-m-pl] 0.4176665095393063\n",
      "fra$solide indéformable$[n-m] 0.36787944117144233\n",
      "fra$solide$[n-m] 0.36787944117144233\n",
      "fra$solde$[n-m-sg] 0.36787944117144233\n",
      "fra$bilan$[n_n-m_n-m-ND] 0.36787944117144233\n",
      "fra$équilibre$[n-m_n_n-m-ND] 0.36787944117144233\n",
      "fra$solidité$[n_n-f] 0.36787944117144233\n",
      "\n",
      "\t\t rus$очень$[adv]\n",
      "fra$fort$[adv] 0.553001792775919\n",
      "fra$durement$[adv] 0.387106962025731\n",
      "fra$quand même$[adv] 0.38619508006205605\n",
      "fra$aussi$[adv] 0.36879132313699686\n",
      "fra$dessus$[adv] 0.36821490379934485\n",
      "fra$très$[adv] 0.36821490379934485\n",
      "fra$fortement$[adv] 0.36821490379934485\n",
      "fra$plus$[adv] 0.36800285097552904\n",
      "fra$en plus$[adv] 0.36800285097552904\n",
      "fra$hautement$[adv] 0.36800285097552904\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "word = 'очень'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rus$тип$[n-m-nn]\n",
      "all:  2829\tfiltered:  48\n",
      "rus$тип$[n-m-aa_n]\n",
      "all:  8642\tfiltered:  186\n",
      "\n",
      "\t\t rus$тип$[n-m-nn]\n",
      "fra$tarif$[n-m_n-m-ND] 0.4176665095393063\n",
      "fra$taxe$[n-f_n_n-f-ND] 0.3861950800601765\n",
      "fra$formulaire$[n-m_n_n-m-ND] 0.3746173881705278\n",
      "fra$indice$[n_n-m_n-m-ND] 0.3746173881705278\n",
      "fra$rythme$[n-m_n_n-m-ND] 0.3746173881705278\n",
      "fra$taux$[n-m-sp_n-m_n] 0.3746173881705278\n",
      "fra$index$[n-m-sp_n-m_n] 0.3746173881705278\n",
      "fra$style$[n_n-m_n-m-ND] 0.36879132313699686\n",
      "fra$aspect$[n-m_n_n-m-ND] 0.36800285097552904\n",
      "fra$forme$[n-f_n_n-f-ND] 0.3679248411012048\n",
      "\n",
      "\t\t rus$тип$[n-m-aa_n]\n",
      "fra$phylum$[n-m] 0.8710941655794974\n",
      "fra$mec$[n_n-m_n-m-ND] 0.7424968293419701\n",
      "fra$espèce$[n-f_n_n-f-ND] 0.5032147244081486\n",
      "fra$zèbre$[n-f] 0.4176665095393063\n",
      "fra$nature$[n-f_n_n-f-ND] 0.3746173881705278\n",
      "fra$fil$[n-m_n_n-m-ND] 0.3703581933481087\n",
      "fra$personne$[n_n-f_n-f-ND] 0.3703581933481087\n",
      "fra$caractère$[n_n-m_n-m-ND] 0.36821490379934485\n",
      "fra$modèle$[n_n-m_n-m-ND] 0.3678855853837957\n",
      "fra$sorte$[n-f_n-f-ND_n] 0.13534142744896602\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "word = 'тип'\n",
    "%time print_results(lemma_search3 (G, word, l1, 'fra', 4), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_languages('spa', 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time load_file('spa', 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_search(node, l2, cutoff):\n",
    "    if node not in G.nodes():\n",
    "        return None\n",
    "    s = FilteredList(nx.single_source_shortest_path_length(G, node, cutoff=cutoff))\n",
    "    results = {}\n",
    "    k = 0\n",
    "    if len(s) > 100:\n",
    "        s = s.lang(l2)\n",
    "        if len(s) > 5 :s = s[:20]\n",
    "    else:\n",
    "        while k < 8:\n",
    "            s = FilteredList(nx.single_source_shortest_path_length(G, node, cutoff=cutoff+k))\n",
    "            s = s.lang(l2)\n",
    "            if len(s) > 5:\n",
    "                s = s[:20]\n",
    "                break\n",
    "            else: k += 1#translations(G, word, cutoff)\n",
    "        for translation in s:\n",
    "            t = Counter([len(i) for i in nx.all_simple_paths(G, node, translation, cutoff=cutoff+k)])\n",
    "            coef = 0\n",
    "            for i in t: coef += exp(-t[i])\n",
    "            results[translation] = coef\n",
    "    return list(sorted(results, key=results.get, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q', 'abba', 'a']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {'abba': 10, 'a':1, 'q':90}\n",
    "list(sorted(results, key=results.get, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_node_search (G, node1, node2, l1, l2, cutoff):\n",
    "    if (node1, node2) in G.edges(): G.remove_edge(node1, node2)\n",
    "    if (node2, node1) in G.edges(): G.remove_edge(node2, node1)\n",
    "    res1 = node_search(node1, l2, cutoff)\n",
    "    res2 = node_search(node2, l1, cutoff)\n",
    "    coefficient = 0\n",
    "    if node2 in res1: coefficient += 0.5*(len(res1) - res1.index(node2))/len(res1)\n",
    "    if node1 in res2: coefficient += 0.5*(len(res2) - res2.index(node1))/len(res2) \n",
    "    if not res1: res1 = ['-']  \n",
    "    if not res2: res2 = ['-']    \n",
    "    #print (node1.lemma, node2.lemma, coefficient, res1[0], res2[0]) \n",
    "    #print ('\\t', res1)\n",
    "    #print ('\\t', res2)\n",
    "    return coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_encoding('spa-eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "del G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 26s\n"
     ]
    }
   ],
   "source": [
    "%time G = built_from_file('spa-eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.75 s\n"
     ]
    }
   ],
   "source": [
    "%time l1, l2 = dictionaries('spa', 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4327082 1945709\n"
     ]
    }
   ],
   "source": [
    "print (len(G.edges()), len(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "candidates = random.sample(l1, 10000)\n",
    "pairs = []\n",
    "for i in candidates:\n",
    "    if i in G.nodes():\n",
    "        s = FilteredList(list(G.neighbors(i))).lang('eng')\n",
    "        if (i.lemma.capitalize() != i.lemma) and (G.degree(i) > 4) and (not ' ' in i.lemma):\n",
    "            if len(s) == 1:\n",
    "                pairs.append((i, s[0]))\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs2 = pairs[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs[90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(G, pairs, l1, l2, cutoff):\n",
    "    result = []\n",
    "    for i in pairs:\n",
    "        result.append(two_node_search (G, i[0], i[1], l1, l2, cutoff))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%time result = evaluate(G, pairs2, 'spa', 'eng', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(spa$ante$[pr], eng$in front of$[pr]),\n",
       " (spa$delicadeza$[n_n-f], eng$delicacy$[n_n-ND]),\n",
       " (spa$nihilista$[adj_adj-mf-sg_adj-mf], eng$nihilistic$[adj]),\n",
       " (spa$reponer$[vblex], eng$replace$[vblex]),\n",
       " (spa$civilmente$[adv], eng$civilly$[adv]),\n",
       " (spa$inocentemente$[adv], eng$innocently$[adv]),\n",
       " (spa$radiar$[vblex], eng$irradiate$[vblex]),\n",
       " (spa$ecuador$[n-m_n_n-m-sg], eng$equator$[n_n-sg]),\n",
       " (spa$anales$[n_n-m], eng$annals$[n_n-pl]),\n",
       " (spa$depositario$[adj_adj-m-sg], eng$depository$[adj]),\n",
       " (spa$inexorablemente$[adv], eng$inexorably$[adv]),\n",
       " (spa$procedencia$[n_n-f_n-f-sg], eng$origin$[n_n-ND]),\n",
       " (spa$transcribir$[vblex], eng$transcribe$[vblex]),\n",
       " (spa$proletarización$[n_n-f], eng$proletarianisation$[n]),\n",
       " (spa$misticismo$[n_n-m], eng$mysticism$[n_n-sg]),\n",
       " (spa$resentimiento$[n_n-m], eng$resentment$[n_n-ND]),\n",
       " (spa$saúco$[n_n-m], eng$elder$[n_n-ND]),\n",
       " (spa$amarillento$[adj_adj-m-sg_adj-m], eng$yellowish$[adj]),\n",
       " (spa$mestizaje$[n-m_n], eng$miscegenation$[n]),\n",
       " (spa$discípulo$[n_n-m_n-m-sg], eng$disciple$[n_n-sg]),\n",
       " (spa$administrador$[n_n-m_n-m-sg], eng$administrator$[n_n-ND]),\n",
       " (spa$salvación$[n_n-f], eng$salvation$[n_n-ND]),\n",
       " (spa$cerca$[pr], eng$near$[pr]),\n",
       " (spa$voluntario$[n_n-GD], eng$volunteer$[n_n-ND]),\n",
       " (spa$suegro$[n_n-m_n-m-pl], eng$father-in-law$[n_n-ND]),\n",
       " (spa$propenso$[adj_adj-m-sg], eng$liable$[adj]),\n",
       " (spa$DVD$[n_n-acr-m_n-acr], eng$DVD$[n-acr_n_n-acr-sg]),\n",
       " (spa$enumerar$[vblex], eng$enumerate$[vblex]),\n",
       " (spa$eructo$[n_n-m], eng$eructation$[n]),\n",
       " (spa$demonio$[n_n-m], eng$demon$[n_n-sg]),\n",
       " (spa$tamarindo$[n_n-m], eng$tamarind$[n]),\n",
       " (spa$ingesta$[n_n-f], eng$consumption$[n_n-ND]),\n",
       " (spa$tenor$[n_n-m_n-m-sg], eng$tenor$[n]),\n",
       " (spa$confundir$[vblex], eng$confuse$[vblex]),\n",
       " (spa$analfabeto$[n_n-GD], eng$illiterate$[n]),\n",
       " (spa$silbido$[n-m_n], eng$whistle$[n]),\n",
       " (spa$justificación$[n_n-f_n-f-sg], eng$justification$[n_n-ND]),\n",
       " (spa$condicionante$[n_n-m], eng$conditioning$[n_n-ND]),\n",
       " (spa$democráticamente$[adv], eng$democratically$[adv]),\n",
       " (spa$descomposición$[n_n-f], eng$decomposition$[n]),\n",
       " (spa$enigmático$[adj_adj-m-sg_adj-m], eng$enigmatic$[adj]),\n",
       " (spa$inventario$[n-m_n_n-m-sg], eng$inventory$[n_n-ND]),\n",
       " (spa$trapo$[n-m_n_n-m-sg], eng$cloth$[n_n-pl_]),\n",
       " (spa$reforma$[n_n-f_n-f-sg], eng$reform$[n_n-ND]),\n",
       " (spa$atrapamiento$[n_n-m], eng$entrapment$[n]),\n",
       " (spa$peyorativo$[adj_adj-m-sg], eng$pejorative$[adj]),\n",
       " (spa$evolucionar$[vblex], eng$evolve$[vblex]),\n",
       " (spa$flexibilidad$[n_n-f_n-f-sg], eng$flexibility$[n_n-ND]),\n",
       " (spa$pujanza$[n_n-f], eng$strength$[n_n-sg]),\n",
       " (spa$palacio$[n-m_n_n-m-ND], eng$palace$[n_n-ND]),\n",
       " (spa$indisciplina$[n_n-f], eng$indiscipline$[n]),\n",
       " (spa$geométricamente$[adv], eng$geometrically$[adv]),\n",
       " (spa$destinar$[vblex], eng$allocate$[vblex]),\n",
       " (spa$carpintero$[n_n-m], eng$carpenter$[n_n-sg]),\n",
       " (spa$ocupado$[adj_adj-GD-ND], eng$busy$[adj-sint_adj]),\n",
       " (spa$lingüística$[n_n-f_n-f-sg], eng$linguistics$[n_n-sg]),\n",
       " (spa$colectivismo$[n_n-m], eng$collectivism$[n_n-unc]),\n",
       " (spa$inalterable$[adj_adj-mf_adj-mf-sg], eng$unalterable$[adj]),\n",
       " (spa$colosal$[adj_adj-mf_adj-mf-sg], eng$colossal$[adj]),\n",
       " (spa$recriminación$[n_n-f], eng$recrimination$[n]),\n",
       " (spa$pomo$[n-m_n], eng$knob$[n]),\n",
       " (spa$incontenible$[adj-mf_adj], eng$irrepressible$[adj]),\n",
       " (spa$opinar$[vblex], eng$think$[vblex]),\n",
       " (spa$linfa$[n_n-f], eng$lymph$[n_n-sg]),\n",
       " (spa$redimir$[vblex], eng$redeem$[vblex]),\n",
       " (spa$limusina$[n_n-f], eng$limousine$[n]),\n",
       " (spa$notoriamente$[adv], eng$notoriously$[adv]),\n",
       " (spa$retórico$[adj_adj-m-sg], eng$rhetorical$[adj]),\n",
       " (spa$exhaustivamente$[adv], eng$thoroughly$[adv]),\n",
       " (spa$anisotropía$[n-f_n], eng$anisotropy$[n]),\n",
       " (spa$resoluble$[adj_adj-mf], eng$solvable$[adj]),\n",
       " (spa$alternativa$[n_n-f_n-f-sg], eng$alternative$[n_n-ND]),\n",
       " (spa$plural$[n-m_n], eng$plural$[n_n-ND]),\n",
       " (spa$denotación$[n_n-f], eng$denotation$[n]),\n",
       " (spa$beatificación$[n_n-f], eng$beatification$[n_n-unc]),\n",
       " (spa$islamista$[adj_adj-mf_adj-mf-sg], eng$islamist$[adj]),\n",
       " (spa$pampa$[n_n-f], eng$pampa$[n_n-ND]),\n",
       " (spa$corporativamente$[adv], eng$corporately$[adv]),\n",
       " (spa$espalda$[n-f_n_n-f-pl], eng$back$[n_n-ND]),\n",
       " (spa$banderín$[n-m_n], eng$pennant$[n]),\n",
       " (spa$tolerable$[adj_adj-mf-sg_adj-mf], eng$tolerable$[adj]),\n",
       " (spa$podología$[n_n-f], eng$podiatry$[n]),\n",
       " (spa$espectáculo$[n-m_n], eng$show$[n_n-ND]),\n",
       " (spa$seleccionable$[adj_adj-mf], eng$selectable$[adj]),\n",
       " (spa$corporación$[n_n-f], eng$corporation$[n_n-ND]),\n",
       " (spa$vibrar$[vblex], eng$vibrate$[vblex]),\n",
       " (spa$brisa$[n_n-f_n-f-sg], eng$breeze$[n_n-ND]),\n",
       " (spa$altruista$[adj_adj-mf_adj-mf-sg], eng$altruistic$[adj]),\n",
       " (spa$personalizado$[adj], eng$personalised$[adj]),\n",
       " (spa$sándalo$[n_n-m], eng$sandal$[n]),\n",
       " (spa$lira$[n-f_n], eng$lyre$[n_n-ND]),\n",
       " (spa$eslabón$[n-m_n], eng$link$[n_n-pl]),\n",
       " (spa$dracma$[n-f_n], eng$drachma$[n]),\n",
       " (spa$católico$[n_n-m], eng$Catholic$[n_n-ND]),\n",
       " (spa$cacofónico$[adj_adj-m-sg], eng$cacophonic$[adj]),\n",
       " (spa$frecuentemente$[adv], eng$frequently$[adv]),\n",
       " (spa$adjudicación$[n_n-f_n-f-sg], eng$adjudication$[n]),\n",
       " (spa$apicultura$[n_n-f], eng$apiculture$[n]),\n",
       " (spa$megabit$[n_n-m], eng$megabit$[n]),\n",
       " (spa$diligente$[adj-mf_adj_adj-mf-sg], eng$diligent$[adj])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.285588023088025"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(result)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
