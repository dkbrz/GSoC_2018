{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, os, requests, json, re\n",
    "from collections import Counter\n",
    "from math import exp, log10\n",
    "from itertools import islice\n",
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET\n",
    "from github import Github\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',level=logging.INFO, stream=sys.stdout)\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "from heapdict import heapdict\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np, scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, lemma, lang, s=[]):\n",
    "        if lemma == None: self.lemma = ''\n",
    "        else: self.lemma = lemma\n",
    "        self.lang = lang\n",
    "        self.s = s\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.s:\n",
    "            if isinstance(self.s[0],list): w = '['+'_'.join(['-'.join(i) for i in self.s])+']'\n",
    "            else: w = '['+'-'.join(self.s)+']'\n",
    "        else: w = '-'\n",
    "        return str(self.lang)+'$'+str(self.lemma)+'$'+str(w)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.lemma == other.lemma and self.lang == other.lang and (self.s == other.s or other.s in self.s or self.s in other.s)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if self.lang == other.lang and self.lemma == other.lemma:\n",
    "            s1 = set(self.s)\n",
    "            s2 = set(other.s)\n",
    "            if (not s1 - s2) and (s1&s2==s1) and (s2 - s1): return True\n",
    "            else: return False\n",
    "        else: return False\n",
    "    \n",
    "    def __hash__(self): return hash(str(self))\n",
    "    \n",
    "    def write(self, mode='mono'):\n",
    "        \"Mono: format to write in monodix, bi: format to write in bidix\"\n",
    "        if mode == 'mono': return self.lemma + '\\t' + '$'.join([str(i) for i in self.s])\n",
    "        elif mode == 'bi': return self.lang + '\\t' +  self.lemma + '\\t' + '$'.join([str(i) for i in self.s])\n",
    "              \n",
    "class Tags(list):\n",
    "    def __le__(self, other):\n",
    "        s1 = set(self)\n",
    "        s2 = set(other)\n",
    "        if not s1 - s2 and s1&s2==s1: return True\n",
    "        else: return False\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        s1 = set(self)\n",
    "        s2 = set(other)\n",
    "        if (not s1 - s2) and (s1&s2==s1) and (s2 - s1): return True\n",
    "        else: return False\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if set(self) == set(other): return True\n",
    "        else: return False\n",
    "        \n",
    "    def __str__(self): return '-'.join(self)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __hash__(self): return hash(str(self))\n",
    "    \n",
    "class WordDict(dict):\n",
    "    def lemma(self, lemma): self.lemma = lemma\n",
    "        \n",
    "class FilteredDict(dict):\n",
    "    def set_lang(self, lang): self.lang = lang\n",
    "    \n",
    "    def lemma(self, lemma): return self[self.lang+'_'+lemma]\n",
    "        \n",
    "    def add(self, word):\n",
    "        lemma = word.lang+'_'+word.lemma\n",
    "        tags = Tags(word.s)\n",
    "        if lemma in self:\n",
    "            if tags in self[lemma]: self[lemma][tags] += 1\n",
    "            else: self[lemma][tags] = 1\n",
    "        else:\n",
    "            self[lemma] = WordDict()\n",
    "            self[lemma].lemma(lemma)\n",
    "            self[lemma][tags] = 1\n",
    "            \n",
    "class DiGetItem:\n",
    "    def __init__(self):\n",
    "        self.list = []\n",
    "        self.dict = {}\n",
    "    \n",
    "    def add(self, word):\n",
    "        if len (word.s) > 1: self.list.append(word)\n",
    "        else: self.dict[word] = word\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        key2 = Word(key.lemma, key.lang, [''])\n",
    "        if key in self.dict: return self.dict[key]\n",
    "        else:\n",
    "            if key2 in self.dict: return self.dict[key2]\n",
    "            try:\n",
    "                key = self.list[self.list.index(key)]\n",
    "                return key\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def __len__(self): return len(self.list)+len(self.dict)\n",
    "                \n",
    "class SetWithFilter(set):\n",
    "    def lemma(self, value): return set(i for i in self if i.lemma == value)\n",
    "    def lang(self, value): return set(i for i in self if i.lang == value)\n",
    "\n",
    "class FilteredList(list):\n",
    "    def lemma(self, value): return list(i for i in self if i.lemma == value)\n",
    "    def lang(self, value): return list(i for i in self if i.lang == value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, os, re, requests\n",
    "from github import Github\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',level=logging.INFO, stream=sys.stdout)\n",
    "from tool.data import lang_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-14 09:06:36,240 | INFO : Start\n",
      "2018-07-14 09:17:23,238 | INFO : Finish\n"
     ]
    }
   ],
   "source": [
    "download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(lang, mode=3):\n",
    "    \"Language code converter\"\n",
    "    if lang in lang_codes: return lang_codes[lang]\n",
    "    else: return lang\n",
    "\n",
    "def repo_names(user):\n",
    "    \"List of language pair repos in Apertium\"\n",
    "    for repo in user.get_repos():\n",
    "        if re.match('apertium-[a-z]{2,3}(_[a-zA-Z]{2,3})?-[a-z]{2,3}(_[a-zA-Z]{2,3})?', repo.name):\n",
    "            yield repo.name\n",
    "\n",
    "def bidix_url(repo):\n",
    "    \"Find raw url for bidix. Sorting in order to find bidix faster as it is one of the longest filename in repo\"\n",
    "    for i in sorted(repo.get_dir_contents('/'), key = lambda x: (len(x.path), 1000-ord(('   '+x.path)[-3])), reverse=True):\n",
    "        if re.match('apertium-.*?\\.[a-z]{2,3}(_[a-zA-Z]{2,3})?-[a-z]{2,3}(_[a-zA-Z]{2,3})?.dix$', i.path): return i.download_url\n",
    "        elif len(i.path) < 23: return None\n",
    "        \n",
    "def download():\n",
    "    from tool.secure import SECRET\n",
    "    github = Github(SECRET['USER'], SECRET['PASSWORD'])  #import username and password\n",
    "    user = github.get_user('apertium')\n",
    "    \n",
    "    logging.info('Start')\n",
    "    if not os.path.exists('./dictionaries/'): os.makedirs('./dictionaries/')\n",
    "    for repo_name in repo_names(user):\n",
    "        bidix = bidix_url(github.get_repo(user.name+'/'+repo_name))\n",
    "        if bidix:\n",
    "            filename = './dictionaries/'+bidix.split('/')[-1]\n",
    "            response = requests.get(bidix)\n",
    "            response.encoding = 'UTF-8'\n",
    "            with open(filename, 'w', encoding='UTF-8') as f: f.write(response.text)\n",
    "    logging.info('Finish')  \n",
    "\n",
    "def set_github_user(user, password):\n",
    "    with open ('./tool/secure.py', 'w', encoding='utf-8') as f:\n",
    "        f.write('SECRET = {\"USER\": \"'+user+'\", \"PASSWORD\": \"'+password+'\"}')\n",
    "\n",
    "def list_files(path='./dictionaries/'):\n",
    "    from tool.data import remove\n",
    "    with open ('filelist.txt','w', encoding='utf-8') as f:\n",
    "        for root, dirs, files in os.walk (path):\n",
    "            for file in files:\n",
    "                if re.match('apertium-.*?\\.[a-z]{2,3}(_[a-zA-Z]{2,3})?-[a-z]{2,3}(_[a-zA-Z]{2,3})?.dix$', file):\n",
    "                    name = '-'.join(l(i) for i in file.split('.')[-2].split('-'))\n",
    "                    if name not in remove:\n",
    "                        f.write(os.path.abspath(os.path.join(root, file)).replace(\"\\\\\",\"/\")+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monodix():\n",
    "    \"Create all monodixes and add them to monodix folder\"\n",
    "    all_languages()\n",
    "    logging.info('Start')\n",
    "    if not os.path.exists('./monodix/'):\n",
    "        os.makedirs('./monodix/')\n",
    "    for lang in langs:\n",
    "        dictionary = one_language_dict(lang)\n",
    "        with open ('./monodix/'+lang+'.dix', 'w', encoding = 'utf-16') as f:\n",
    "            for i in dictionary_to_nodes(dictionary):\n",
    "                f.write (i.write(mode='mono')+'\\n')\n",
    "    logging.info('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_languages():\n",
    "    s = set()\n",
    "    with open ('./tool/langs.py','w',encoding='utf-8') as outp:\n",
    "        with open ('filelist.txt','r',encoding='utf-8') as inp:\n",
    "            for line in inp:\n",
    "                name = [l(i) for i in line.split('.')[-2].split('-')]\n",
    "                s.update(name)\n",
    "        outp.write('langs='+str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tool.langs import langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidix():\n",
    "    with open ('./tool/stats.csv','w',encoding='utf-8') as stats:\n",
    "        with open('./filelist.txt', 'r', encoding = 'utf-8') as f:\n",
    "            for line in f:\n",
    "                file = line.strip('\\n')\n",
    "                name = [l(i) for i in line.split('.')[-2].split('-')]\n",
    "                if '-'.join(name) in rename: name = rename['-'.join(pair)]\n",
    "                with open (file, 'r', encoding='utf-8') as d:\n",
    "                    with open ('./parsed/'+'-',join(name), 'w', encoding='utf-8') as copy:\n",
    "                        count = [0,0,0]\n",
    "                        try:\n",
    "                            tree = ET.fromstring(re.sub('\\s{3,}','\\t', d.read().replace('<b/>',' ').replace('<.?g>','')))\n",
    "                            for word1, word2, side in parse_bidix (tree, name[0], self.name[1]):\n",
    "                                try:\n",
    "                                    word1, word2 = check (word1, word2, self.mono1, self.mono2)\n",
    "                                    if not side: count[0]+=1\n",
    "                                    elif side == 'LR': count[1] += 1\n",
    "                                    elif side == 'RL': count[2] += 1\n",
    "                                    string = str(side) + '\\t' + word1.write(mode='bi') + '\\t' + word2.write(mode='bi') + '\\n'\n",
    "                                    copy.write(string)\n",
    "                                except: pass\n",
    "                        except: pass\n",
    "                    stats.write('\\t'.join(name) + '\\t'+ '\\t'.join(count)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    all_languages()\n",
    "    from tool.langs import langs\n",
    "    monodix()\n",
    "    bidix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tool.func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-14 10:22:35,035 | INFO : Start\n",
      "2018-07-14 10:25:59,821 | INFO : Finish\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './parsed/afr-nld'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cf1287f81991>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\GSoC_2018\\tool\\func.py\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m()\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlangs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlangs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[0mmonodix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mbidix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_mono\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\GSoC_2018\\tool\\func.py\u001b[0m in \u001b[0;36mbidix\u001b[1;34m()\u001b[0m\n\u001b[0;32m    332\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrename\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m                     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'./parsed/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m                         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './parsed/afr-nld'"
     ]
    }
   ],
   "source": [
    "preprocessing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
