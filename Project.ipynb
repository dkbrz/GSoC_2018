{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilingual dictionary enrichment via graph completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import na\n",
    "import pandas as pd\n",
    "lang_codes = pd.read_csv('./files/language-codes-full_csv.csv', na_values = 0)\n",
    "lang_codes = lang_codes[['alpha3-b','alpha2']]\n",
    "lang_codes = lang_codes.dropna()\n",
    "\n",
    "\n",
    "lang_codes = [{i[0]:i[1] for i in np.array(lang_codes)}, {i[1]:i[0] for i in np.array(lang_codes)}]\n",
    "\n",
    "with open ('./files/lang_codes.json', 'w') as f:\n",
    "    json.dump(lang_codes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tat'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('./files/lang_codes.json', 'r') as f:\n",
    "    lang_codes = json.load(f)\n",
    "\n",
    "def l(lang, mode=3):\n",
    "    mode = mode % 2\n",
    "    if len(lang)==2:\n",
    "        if lang in lang_codes[mode]:\n",
    "            return lang_codes[mode][lang]\n",
    "        else:\n",
    "            return lang\n",
    "    else:\n",
    "        return lang\n",
    "l('tt', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyGithub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load user with login and password from secret file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "with open ('secure.json') as f:\n",
    "    SECRET = json.loads(f.read())\n",
    "\n",
    "github = Github(SECRET['USER'], SECRET['PASSWORD'])\n",
    "\n",
    "user = github.get_user('apertium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user.get_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Generator ** : yield all repos that match name pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repo_names(user):\n",
    "    for repo in user.get_repos():\n",
    "        if re.match('apertium-[a-z]{2,3}(_[a-zA-Z]{2,3})?-[a-z]{2,3}(_[a-zA-Z]{2,3})?', repo.name):\n",
    "            yield repo.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like heavy function. But I don't see any improvements yet, except for having certain repo for all bidix copies. But this one above is the most up-to-date. It filters not languages pair repos, it is needed not to look for bidix where it can't be. Function saves a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%time w = list(repo_names(user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find bidix **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length sorting to reduce number of files to check (bidix is lone of the longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bidix_url(repo):\n",
    "    for i in sorted(repo.get_dir_contents('/'), key = lambda x: (len(x.path), 1000-ord(('   '+x.path)[-3])), reverse=True):\n",
    "        if re.match('apertium-.*?\\.[a-z]{2,3}(_[a-zA-Z]{2,3})?-[a-z]{2,3}(_[a-zA-Z]{2,3})?.dix$', i.path):\n",
    "            return i.download_url\n",
    "        elif len(i.path) < 23:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 709 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://raw.githubusercontent.com/apertium/apertium-cat-srd/master/apertium-cat-srd.cat-srd.dix'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time bidix_url(github.get_repo(user.name+'/'+w[22]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Only relevant for certain language pair **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are **164 ** pairs at this moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_all_bidixes(user):\n",
    "    logging.info('Start')\n",
    "    if not os.path.exists('./dictionaries/'):\n",
    "        os.makedirs('./dictionaries/')\n",
    "    for repo_name in repo_names(user):\n",
    "        bidix = bidix_url(github.get_repo(user.name+'/'+repo_name))\n",
    "        langs = [l(i) for i in repo_name.split('-')[1:]]\n",
    "        filename = './dictionaries/'+'-'.join(langs)+'.dix'\n",
    "        if bidix:\n",
    "            response = requests.get(bidix)\n",
    "            response.encoding = 'UTF-8'\n",
    "            with open(filename, 'w', encoding='UTF-8') as f:\n",
    "                f.write(response.text)\n",
    "    logging.info('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-18 11:22:17,854 | INFO : Start\n",
      "2018-05-18 11:30:11,682 | INFO : Finish\n"
     ]
    }
   ],
   "source": [
    "download_all_bidixes(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_relevant_languages(l1, l2):\n",
    "    G = nx.Graph()\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix', '').split('-')\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    pair = [l(l1), l(l2)]\n",
    "    with open('language_list.csv','w', encoding='utf-8') as f:\n",
    "        nodes = set()\n",
    "        for i in range(1,5):\n",
    "            w = nx.single_source_shortest_path_length(G, pair[0], cutoff=i)\n",
    "            v = nx.single_source_shortest_path_length(G, pair[1], cutoff=i)\n",
    "            H = G.subgraph(w.keys())\n",
    "            H.remove_node(pair[0])\n",
    "            H2 = G.subgraph(v.keys())\n",
    "            H2.remove_node(pair[1])\n",
    "            if pair[1] in H.nodes():\n",
    "                v = nx.node_connected_component(H, pair[1])\n",
    "            else:\n",
    "                v = set()\n",
    "            if pair[0] in H2.nodes():\n",
    "                w = nx.node_connected_component(H, pair[1])\n",
    "            else:\n",
    "                w = set() \n",
    "            nodes2 = v & w | set([pair[0], pair[1]])\n",
    "            nodes2 = nodes2 - nodes\n",
    "            for node in nodes2:\n",
    "                f.write('{}\\t{}\\n'.format(i*2, node))\n",
    "            nodes = nodes | nodes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_relevant_languages('bel', 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_languages():\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files:\n",
    "            #print (root+fl)\n",
    "            try:\n",
    "                s = ET.parse(root+fl)\n",
    "            except:\n",
    "                print ('ERROR :'+fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR :epo-bul.dix\n",
      "ERROR :epo-per.dix\n",
      "ERROR :epo-pol.dix\n",
      "ERROR :fin-fra.dix\n",
      "ERROR :pol-lav.dix\n",
      "ERROR :sah-eng.dix\n"
     ]
    }
   ],
   "source": [
    "check_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def existance(pair, nodes):\n",
    "    if pair[0] in nodes and pair[1] in nodes:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_chosen():\n",
    "    with open ('language_list.csv','r',encoding='utf-8') as f:\n",
    "        languages = set([i.split('\\t')[1].strip() for i in f.readlines()])\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files:\n",
    "            pair = fl.replace('.dix','').split('-')\n",
    "            if existance(pair, languages):\n",
    "                try:\n",
    "                    with open (root+fl, 'r', encoding='utf-8') as d:\n",
    "                        dictionary = d.read().replace('<b/>',' ').replace('<.?g>','')\n",
    "                        yield ET.fromstring(dictionary), pair[0], pair[1]\n",
    "                except:\n",
    "                    print ('ERROR: ', fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:  epo-bul.dix\n",
      "ERROR:  epo-per.dix\n",
      "ERROR:  epo-pol.dix\n",
      "ERROR:  fin-fra.dix\n",
      "ERROR:  pol-lav.dix\n",
      "ERROR:  sah-eng.dix\n",
      "Wall time: 3min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time len(list(load_chosen()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Word **\n",
    "\n",
    "- lemma : lemma\n",
    "- lang : language\n",
    "- pos : part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, lemma, lang, s=[]):\n",
    "        self.lemma = lemma\n",
    "        self.lang = lang\n",
    "        self.s = s\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.lang)+'_'+str(self.lemma)+'_'+str('-'.join(self.s))\n",
    "    \n",
    "    __repr__ = __str__\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.lemma == other.lemma and self.lang == other.lang and self.s == other.s\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidix parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 921 ms\n"
     ]
    }
   ],
   "source": [
    "%time T = tree('https://raw.githubusercontent.com/apertium/apertium-eng-ita/master/apertium-eng-ita.eng-ita.dix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_word(word, lang):\n",
    "    s = word.findall('.//s')\n",
    "    s = [i.attrib['n'] for i in s]\n",
    "    return Word(word.text, lang, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_bidix (tree, l1, l2):\n",
    "    tree = tree.find('section')\n",
    "    if not tree:\n",
    "        print (l1, l2)\n",
    "    else:\n",
    "        for e in tree:\n",
    "            if 'n' in e.attrib:\n",
    "                side = e.attrib['n']\n",
    "            else:\n",
    "                side = None\n",
    "            p = e.find('p')\n",
    "            if p:\n",
    "                yield one_word(p.find('l'), l1), one_word(p.find('r'), l2), side\n",
    "            else:\n",
    "                try:\n",
    "                    i = e.find('i')\n",
    "                    yield one_word(i, l1), one_word(i, l2), side\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 882 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48880"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% time len(list(parse_bidix (T, 'bel','rus')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_bidix(G, T, l1, l2):\n",
    "    for word1, word2, side in parse_bidix (T, l1, l2):\n",
    "        if side == None:\n",
    "            G.add_edge(word1, word2)\n",
    "            G.add_edge(word2, word1)\n",
    "        elif side == 'LR':\n",
    "            G.add_edge(word1, word2)\n",
    "        elif side == 'RL':\n",
    "            G.add_edgr(word2, word1)\n",
    "        else:\n",
    "            print (side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "%time add_bidix(G, T, 'bel', 'rus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 14:31:55,751 | INFO : Start\n",
      "ERROR:  epo-bul.dix\n",
      "ERROR:  epo-per.dix\n",
      "ERROR:  epo-pol.dix\n",
      "ERROR:  fin-fra.dix\n",
      "lit lav\n",
      "ERROR:  pol-lav.dix\n",
      "ERROR:  sah-eng.dix\n",
      "2018-05-20 14:35:14,629 | INFO : Finish\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "logging.info('Start')\n",
    "for T, l1, l2 in load_chosen():\n",
    "    add_bidix(G, T, l1, l2)\n",
    "logging.info('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SetWithFilter(set):\n",
    "    def lemma(self, value):\n",
    "        return set(i for i in self if i.lemma == value)\n",
    "    def pos(self, value):\n",
    "        return set(i for i in self if i.pos == value)\n",
    "    def lang(self, value):\n",
    "        return set(i for i in self if i.lang == value)\n",
    "    def notlang(self, value):\n",
    "        return set(i for i in self if i.lang != value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionaries(G, l1,l2):\n",
    "    l1 = l(l1)\n",
    "    l2 = l(l2)\n",
    "    d_l1 = SetWithFilter()\n",
    "    d_l2 = SetWithFilter()\n",
    "    for i in G.nodes():\n",
    "        if i.lang == l1:\n",
    "            d_l1.add(i)\n",
    "        elif i.lang == l2:\n",
    "            d_l2.add(i)\n",
    "    return d_l1, d_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemma_search (G, lemma, d_l1, l2, cutoff):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    print (lemmas)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    for word in lemmas:\n",
    "        print(word, end='\\t')\n",
    "        s = SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "        print ('all: ', str(len(s)), end='\\t')\n",
    "        s = s.lang(l2)\n",
    "        print ('filtered: ', str(len(s)))\n",
    "        for translation in s:\n",
    "            t = list(nx.all_simple_paths(G, word, translation, cutoff=cutoff))\n",
    "            t = [len(i) for i in t]\n",
    "            t = Counter(t)\n",
    "            coef = 0\n",
    "            for i in t:\n",
    "                coef += exp(-t[i])\n",
    "            results[str(word)][str(translation)] = coef\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    for i in results:\n",
    "        print ('\\n\\t\\t', i)\n",
    "        for j in sorted(results[i], key=results[i].get, reverse=True)[:7]:\n",
    "            print (j, results[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUS-FRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_relevant_languages('rus', 'fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-20 11:26:48,971 | INFO : Start\n",
      "ERROR:  epo-bul.dix\n",
      "ERROR:  epo-per.dix\n",
      "ERROR:  epo-pol.dix\n",
      "ERROR:  fin-fra.dix\n",
      "lit lav\n",
      "ERROR:  pol-lav.dix\n",
      "ERROR:  sah-eng.dix\n",
      "2018-05-20 11:29:36,417 | INFO : Finish\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "logging.info('Start')\n",
    "for T, l1, l2 in load_chosen():\n",
    "    add_bidix(G, T, l1, l2)\n",
    "logging.info('Finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('graph.pkl','wb') as f:\n",
    "    pkl.dump(G, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('graph.pkl','rb') as f:\n",
    "    G = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_l1, d_l2 = dictionaries(G, 'rus','fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_lemma (G, lemma, d_l1, l2):\n",
    "    lemmas = d_l1.lemma(lemma)\n",
    "    print (lemmas)\n",
    "    results = {str(word):{} for word in lemmas}\n",
    "    for word in lemmas:\n",
    "        print (word)\n",
    "        for cutoff in range(1, 8):\n",
    "            print (cutoff, end='\\t')\n",
    "            s = SetWithFilter(nx.single_source_shortest_path_length(G, word, cutoff=cutoff))\n",
    "            print ('all: ', str(len(s)), end='\\t')\n",
    "            s = s.lang(l2)\n",
    "            print ('filtered: ', str(len(s)))\n",
    "            if len(s)>150:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{rus_собака_n-f, rus_собака_n, rus_собака_n-f-aa}\n",
      "rus_собака_n-f\tall:  252\tfiltered:  3\n",
      "rus_собака_n\tall:  800\tfiltered:  20\n",
      "rus_собака_n-f-aa\tall:  2499\tfiltered:  47\n",
      "\n",
      "\t\t rus_собака_n-f\n",
      "fra_chien_n-GD 0.7357588823428847\n",
      "fra_chien_n 0.3861950800601765\n",
      "fra_chien_n-m 0.36787944117144233\n",
      "\n",
      "\t\t rus_собака_n\n",
      "fra_chien_n 0.5032208686204084\n",
      "fra_chien_n-m 0.503214724408055\n",
      "fra_colimaçon_n-m 0.4176665095393063\n",
      "fra_chien_n-GD 0.4176665095393063\n",
      "fra_limaçon_n-m 0.3861950800601765\n",
      "fra_singe_n-m 0.36787944117144233\n",
      "fra_goal_n-m 0.36787944117144233\n",
      "\n",
      "\t\t rus_собака_n-f-aa\n",
      "fra_vigile_n-m 0.503214724408055\n",
      "fra_surveillant_n-f 0.503214724408055\n",
      "fra_surveillant_n-m 0.503214724408055\n",
      "fra_chien_n-GD 0.4180019721672088\n",
      "fra_chien_n 0.3703581933481431\n",
      "fra_chien_n-m 0.36879132313699686\n",
      "fra_haillon_n-m 0.36787944117144233\n",
      "Wall time: 618 ms\n"
     ]
    }
   ],
   "source": [
    "%time print_results(lemma_search (G, 'собака', d_l1, 'fra', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{rus_поле_n, rus_поле_n-nt-nn, rus_поле_n-nt-nn-pl, rus_поле_n-nt}\n",
      "rus_поле_n\tall:  299\tfiltered:  7\n",
      "rus_поле_n-nt-nn\tall:  831\tfiltered:  12\n",
      "rus_поле_n-nt-nn-pl\tall:  3\tfiltered:  0\n",
      "rus_поле_n-nt\tall:  148\tfiltered:  2\n",
      "\n",
      "\t\t rus_поле_n\n",
      "fra_camp_n-m 0.7357588823428847\n",
      "fra_champ_n-m 0.7357588823428847\n",
      "fra_marge_n-m 0.7357588823428847\n",
      "fra_plantation_n-f 0.7357588823428847\n",
      "fra_terrain_n-m 0.36787944117144233\n",
      "fra_berge_n-f 0.1353352832366127\n",
      "fra_rive_n-f 0.1353352832366127\n",
      "\n",
      "\t\t rus_поле_n-nt-nn\n",
      "fra_camp_n-m 0.3746173881705278\n",
      "fra_champ_n-m 0.3746173881705278\n",
      "fra_camp_n 0.36787944117144233\n",
      "fra_terrain_n-m 0.36787944117144233\n",
      "fra_patinoire_n-f 0.36787944117144233\n",
      "fra_cadre_n 0.36787944117144233\n",
      "fra_champ_n 0.36787944117144233\n",
      "\n",
      "\t\t rus_поле_n-nt-nn-pl\n",
      "\n",
      "\t\t rus_поле_n-nt\n",
      "fra_champ_n-m 0.36787944117144233\n",
      "fra_camp_n-m 0.36787944117144233\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%time print_results(lemma_search (G, 'поле', d_l1, 'fra', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{fra_serpent_n-m}\n",
      "fra_serpent_n-m\tall:  767\tfiltered:  10\n",
      "\n",
      "\t\t fra_serpent_n-m\n",
      "rus_змей_n 0.5035501870359576\n",
      "rus_змея_n 0.5035501870359576\n",
      "rus_змейка_n-f-nn 0.3861950800601765\n",
      "rus_змея_n-f-aa 0.37461750070570254\n",
      "rus_уж_n 0.3746173881705278\n",
      "rus_змей_n-m-aa 0.3703581933481087\n",
      "rus_гадюка_n-f-aa 0.36787944117144233\n",
      "Wall time: 6.02 s\n"
     ]
    }
   ],
   "source": [
    "%time print_results(lemma_search (G, 'serpent', d_l2, 'rus', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ben_prpers_prn-p3-infml-aa-mf-sg-gen\n",
      "cym_rhywun_prn-tn-m-sg-tn-m-sg\n",
      "kaz_сіз_prn-pers-p2-sg-frm-gen-subst-nom\n",
      "eng_you're_prn-subj-p2-mf-sp-vbser-pres\n",
      "sco_ye're_prn-subj-p2-mf-sp-vbser-pres\n",
      "fin_sama_adj-pos-sg-ess-n-sg-ess\n",
      "hbs_na_pr-acc-prn-pers-clt-p3-m-sg-acc\n",
      "hin_वह_prn-dem-p3-mf-sg-dst-nom\n",
      "hin_वह_prn-dem-p3-mf-pl-dst-nom\n",
      "ita_Milà_np-cog-cog-cog-cog-mf-sp\n"
     ]
    }
   ],
   "source": [
    "for node in G.nodes():\n",
    "    if len(node.s) > 6:\n",
    "        print (node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in d_l1:\n",
    "    if i.lemma not in d:\n",
    "        d[i.lemma] = set()\n",
    "    d[i.lemma].add('_'.join(i.s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17975050961623681\n"
     ]
    }
   ],
   "source": [
    "a = [' | '.join(list(sorted(d[i]))) for i in d if len(d[i])>1]\n",
    "print(len(a)/len(d_l1))\n",
    "a = Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n | n_m_nn 2841\n",
      "n | n_f_nn 2549\n",
      "n | n_m_aa 1178\n",
      "adj_sint | n 1149\n",
      "n | n_nt_nn 921\n",
      "adj | adj_sint | n 586\n",
      "adj | adj_sint 382\n",
      "n | vblex_perf | vblex_perf_tv 353\n",
      "n | vblex_perf_tv 344\n",
      "n | n_f | n_f_nn 287\n",
      "n | n_m | n_m_nn 265\n",
      "vblex_perf | vblex_perf_tv 251\n",
      "adv | n 236\n",
      "n | vblex_impf 218\n",
      "n | vblex_impf_tv 210\n",
      "n | n_f_aa 197\n",
      "n | vblex_impf | vblex_impf_tv 190\n",
      "adj | n 183\n",
      "vblex_impf | vblex_impf_tv 154\n",
      "n_m_aa | n_m_nn 132\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(a, key=a.get, reverse=True)[:20]:\n",
    "    print (i, a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in d_l2:\n",
    "    if i.lemma not in d:\n",
    "        d[i.lemma] = set()\n",
    "    d[i.lemma].add('_'.join(i.s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = [d[i] for i in d if 'n' in d[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2980614543114543"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)/len([i.lemma for i in d_l2 if 'n' in i.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17001942380570761\n",
      "n | n_f 2496\n",
      "n | n_m 1651\n",
      "np | np_cog_mf_sp 811\n",
      "adj | n 583\n",
      "adj | adj_mf 386\n",
      "np | np_ant | np_ant_m_sg 330\n",
      "np | np_cog 325\n",
      "np | np_loc_f 311\n",
      "np | np_ant | np_ant_f_sg 288\n",
      "adj | n_m 257\n",
      "np | np_ant 230\n",
      "adj | adj_GD | adj_f | adj_m 193\n",
      "adj | n | n_m 172\n",
      "n_f | n_m 166\n",
      "np | np_ant_m_sg 118\n",
      "n | n_mf 117\n",
      "np_ant_f_sg | np_cog_mf_sp 98\n",
      "np | np_cog | np_cog_mf_sp 84\n",
      "adj | n | n_mf 83\n",
      "num | num_mf_sp 83\n"
     ]
    }
   ],
   "source": [
    "a = [' | '.join(list(sorted(d[i]))) for i in d if len(d[i])>1]\n",
    "print(len(a)/len(d_l2))\n",
    "a = Counter(a)\n",
    "for i in sorted(a, key=a.get, reverse=True)[:20]:\n",
    "    print (i, a[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for every bidix\n",
    "- get relevant\n",
    "- try 4 max\n",
    "- exclude pair (skip)\n",
    "- for every word in actual bidix find best translation\n",
    "- compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relevant (l1, l2):\n",
    "    G = nx.Graph()\n",
    "    for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "        for fl in files :\n",
    "            pair = fl.replace('.dix', '').split('-')\n",
    "            G.add_edge(pair[0], pair[1])\n",
    "    pair = [l(l1), l(l2)]\n",
    "    nodes = set()\n",
    "    i = 2\n",
    "    w = nx.single_source_shortest_path_length(G, pair[0], cutoff=i)\n",
    "    v = nx.single_source_shortest_path_length(G, pair[1], cutoff=i)\n",
    "    H = G.subgraph(w.keys())\n",
    "    H.remove_node(pair[0])\n",
    "    H2 = G.subgraph(v.keys())\n",
    "    H2.remove_node(pair[1])\n",
    "    if pair[1] in H.nodes():\n",
    "        v = nx.node_connected_component(H, pair[1])\n",
    "    else:\n",
    "        v = set()\n",
    "    if pair[0] in H2.nodes():\n",
    "        w = nx.node_connected_component(H, pair[1])\n",
    "    else:\n",
    "        w = set() \n",
    "    nodes = v & w | set([pair[0], pair[1]])\n",
    "    if len(nodes) > 20 and len(nodes) < 40:\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg cat 35\n",
      "bel rus 35\n",
      "chv rus 25\n",
      "chv tat 25\n",
      "chv tur 25\n",
      "cos ita 36\n",
      "eus fin 30\n",
      "eus sme 30\n",
      "grn spa 32\n",
      "guc spa 32\n",
      "kir uzb 21\n",
      "kpv fin 27\n",
      "krl olo 26\n",
      "liv fin 26\n",
      "mrj fin 26\n",
      "myv fin 26\n",
      "oci cat 36\n",
      "oci fra 36\n",
      "oci spa 36\n",
      "olo fin 26\n",
      "quz spa 32\n",
      "rum ita 35\n",
      "scn spa 32\n",
      "udm kpv 36\n",
      "udm rus 36\n",
      "wel spa 32\n",
      "zho spa 32\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk ('./dictionaries/'):\n",
    "    for fl in files :\n",
    "        pair = fl.replace('.dix', '').split('-')\n",
    "        one_comparison(pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_dictionaries(l1, l2):\n",
    "    parse_bidix (tree, l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word compare(G, d1, d2, d1_t, d2_t):\n",
    "    for word in d1:\n",
    "        if word in G.nodes():\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_comparison(l1_m, l2_m):\n",
    "    logging.info('Start\\t'+'{}\\t{}'.format(l1, l2))\n",
    "    nodes = relevant (l1_m, l2_m)\n",
    "    G = nx.DiGraph()\n",
    "    logging.info('Start loading')\n",
    "    for T, l1, l2 in load_chosen():\n",
    "        if (l1_m != l1 and l2_m != 1l) and (l2_m != l1 and l1_m != l2)\n",
    "            add_bidix(G, T, l1, l2)\n",
    "    logging.info('Finish loading')\n",
    "    d1_t, d2_t = \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
